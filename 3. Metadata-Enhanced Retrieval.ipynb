{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3 : Metadata-Enhanced Retrieval\n",
    "\n",
    "In the previous notebook, we saw how we can evaluate our RAG application. In this notebook, we'll see how we can use metadata filtering to improve the quality of our retrieval system.\n",
    "\n",
    "We'll do so in 3 parts\n",
    "\n",
    "1. First we'll show how we can use a VLM like GPT-4o to generate metadata for our items\n",
    "2. Then we'll see how we can use this metadata to filter the items we retrieve\n",
    "3. Finally, we'll use this new metadata filtering and quantify the improvement in our retrieval system with regards to recall and MRR.\n",
    "\n",
    "Let's get started!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using VLMs to generate Metadata\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Metadata for Filtering\n",
    "\n",
    "\n",
    "What we want is a way for us to map a user query to a set of metadata fields that we'd like to use to filter out items. We've defined a `taxonomy.yml` ahead of time that defines the metdata fields which we've used to annotate our dataset that we previously ingested.\n",
    "\n",
    "What we'll do is to read in the taxonomy and make sure that our generated metadata matches this taxonomy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.taxonomy import process_taxonomy_file\n",
    "\n",
    "taxonomy_map = process_taxonomy_file(\"./taxonomy.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Tops', 'Bottoms', 'Dresses', 'Outerwear'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxonomy_map.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'product_type': ['T-Shirts',\n",
       "  'Blouses',\n",
       "  'Sweaters',\n",
       "  'Cardigans',\n",
       "  'Tank Tops',\n",
       "  'Hoodies',\n",
       "  'Sweatshirts'],\n",
       " 'attributes': {'Sleeve Length': ['Sleeveless',\n",
       "   'Short Sleeve',\n",
       "   '3/4 Sleeve',\n",
       "   'Long Sleeve'],\n",
       "  'Neckline': ['Crew Neck', 'V-Neck', 'Turtleneck', 'Scoop Neck', 'Cowl Neck'],\n",
       "  'Fit': ['Regular', 'Slim', 'Oversized', 'Cropped']}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxonomy_map[\"Tops\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from pydantic import BaseModel, model_validator, ValidationInfo\n",
    "\n",
    "\n",
    "class Attribute(BaseModel):\n",
    "    name: str\n",
    "    values: list[str]\n",
    "\n",
    "\n",
    "class QueryFilters(BaseModel):\n",
    "    attributes: list[Attribute]\n",
    "    min_price: Optional[float] = None\n",
    "    max_price: Optional[float] = None\n",
    "    category: str\n",
    "    product_type: list[str]\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate_attributes(self, info: ValidationInfo):\n",
    "        taxonomy_data = info.context[\"taxonomy_data\"]\n",
    "        # Validate category exists in taxonomy\n",
    "        if self.category not in taxonomy_data:\n",
    "            raise ValueError(\n",
    "                f\"Invalid category: {self.category}. Valid categories are {taxonomy_data.keys()}\"\n",
    "            )\n",
    "\n",
    "        # Validate product types\n",
    "        valid_types = taxonomy_data[self.category][\"product_type\"]\n",
    "        for product_type in self.product_type:\n",
    "            if product_type not in valid_types:\n",
    "                raise ValueError(\n",
    "                    f\"Invalid product type: {product_type}. Valid product types are {valid_types}\"\n",
    "                )\n",
    "\n",
    "        # Validate attribute exists in taxonomy\n",
    "        valid_attrs = taxonomy_data[self.category][\"attributes\"]\n",
    "        for attr in self.attributes:\n",
    "            if attr.name not in valid_attrs:\n",
    "                raise ValueError(f\"Invalid attribute name: {attr.name}\")\n",
    "            for value in attr.values:\n",
    "                if value not in valid_attrs[attr.name]:\n",
    "                    raise ValueError(\n",
    "                        f\"Invalid value {value} for attribute {attr.name}. Valid values are {valid_attrs[attr.name]}\"\n",
    "                    )\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll then use this to make a LLM call to generate these metadata fields for our queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QueryFilters</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">attributes</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Attribute</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Sleeve Length'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">values</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Sleeveless'</span><span style=\"font-weight: bold\">])]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">min_price</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">max_price</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100.0</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">category</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Tops'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">product_type</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Tank Tops'</span><span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mQueryFilters\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mattributes\u001b[0m=\u001b[1m[\u001b[0m\u001b[1;35mAttribute\u001b[0m\u001b[1m(\u001b[0m\u001b[33mname\u001b[0m=\u001b[32m'Sleeve Length'\u001b[0m, \u001b[33mvalues\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'Sleeveless'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[33mmin_price\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mmax_price\u001b[0m=\u001b[1;36m100\u001b[0m\u001b[1;36m.0\u001b[0m,\n",
       "    \u001b[33mcategory\u001b[0m=\u001b[32m'Tops'\u001b[0m,\n",
       "    \u001b[33mproduct_type\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'Tank Tops'\u001b[0m\u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import instructor\n",
    "from helpers.taxonomy import process_taxonomy_file\n",
    "from rich import print\n",
    "\n",
    "client = instructor.from_openai(OpenAI())\n",
    "taxonomy_data = process_taxonomy_file(\"taxonomy.yml\")\n",
    "query = \"I want a Tank-Top that's got a short sleeve or sleeveless which is under 100 bucks for an interview\"\n",
    "\n",
    "\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"\n",
    "          You are a helpful assistant that extracts user requirements from a query.\n",
    "                    \n",
    "Use these references:\n",
    "- Taxonomy: {{ taxonomy }}\n",
    "\n",
    "Guidelines:\n",
    "- If a filter isn't needed, return an empty list\n",
    "- Only add attributes and filters that a user has mentioned explicitly\n",
    "- Only use values from the provided taxonomy. \n",
    "- If the attribute exists on multiple types, make sure that you only look at the specific types listed under the category you have chosen\n",
    "- If the user hasn't mentioned a specific product type, lets just use all of them\n",
    "- if the user gives a range (Eg. around 50), just give a buffer of 20 on each side (Eg. 30-70)\n",
    "- if the user gives a vague price (Eg. I have a high budget), just set max price to 1000\n",
    "- only classify an item as unisex if the user has explicitly mentioned it and default to Women's categories by default.\n",
    "- If you're looking at blouses, make sure to include tank tops along the way and vice versa\n",
    "- if the user mentions user bottoms and doesn't specify a specific length - let's include both short and long bottoms such as jeans, shorts and pants\n",
    "- make sure to look carefully at the user's query to determine if they've specified a specific fit - eg. regular, relaxed, cropped. ( Relaxed and Relaxed should always go together)\n",
    "\n",
    "\n",
    "Extract the requirements and format them according to the QueryFilters model.\n",
    "            \"\"\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "    ],\n",
    "    context={\n",
    "        \"taxonomy_data\": taxonomy_data,\n",
    "    },\n",
    "    response_model=QueryFilters,\n",
    ")\n",
    "\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then see a few more other sentences to make sure that the filters are working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QueryFilters</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">attributes</span>=<span style=\"font-weight: bold\">[]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">min_price</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">max_price</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">40.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">category</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Bottoms'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">product_type</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Skirts'</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mQueryFilters\u001b[0m\u001b[1m(\u001b[0m\u001b[33mattributes\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mmin_price\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33mmax_price\u001b[0m=\u001b[1;36m40\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33mcategory\u001b[0m=\u001b[32m'Bottoms'\u001b[0m, \u001b[33mproduct_type\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'Skirts'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import instructor\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "\n",
    "async def extract_query_filters(\n",
    "    client: instructor.AsyncInstructor, query: str\n",
    ") -> QueryFilters:\n",
    "    \"\"\"\n",
    "    Extract structured filters from a natural language query using LLM.\n",
    "\n",
    "    Args:\n",
    "        query (str): Natural language query from user\n",
    "\n",
    "    Returns:\n",
    "        QueryFilters: Structured filters extracted from the query\n",
    "    \"\"\"\n",
    "    return await client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant that extracts user requirements from a query. Refer to this taxonomy for valid categories, subcategories, product types and attributes: {{ taxonomy_data }}. If a filter isn't needed, just return an empty list. If the user is looking for a specific attribute, just return the attribute name and the values that the user is looking for\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": query},\n",
    "        ],\n",
    "        context={\n",
    "            \"taxonomy_data\": taxonomy_data,\n",
    "        },\n",
    "        response_model=QueryFilters,\n",
    "    )\n",
    "\n",
    "\n",
    "client = instructor.from_openai(AsyncOpenAI())\n",
    "print(await extract_query_filters(client, \"Need a new skirt around 40 bucks\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see how we can use this metadata filtering with LanceDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from lancedb import connect\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def retrieve_and_filter(query: str, table, filters: QueryFilters, max_k=75):\n",
    "    query_parts = []\n",
    "\n",
    "    # We do a prefilter on category,price and material since these will always be provided\n",
    "    query_parts.append(f\"category='{filters.category}'\")\n",
    "\n",
    "    if filters.min_price:\n",
    "        query_parts.append(f\"price >= {filters.min_price}\")\n",
    "    if filters.max_price:\n",
    "        query_parts.append(f\"price <= {filters.max_price}\")\n",
    "\n",
    "    query_string = \" AND \".join(query_parts)\n",
    "    items = (\n",
    "        table.search(query=query)\n",
    "        .where(query_string, prefilter=True)\n",
    "        .limit(max_k)\n",
    "        .to_list()\n",
    "    )\n",
    "\n",
    "    items = [\n",
    "        {\n",
    "            **item,\n",
    "            \"attributes\": json.loads(item[\"attributes\"]),\n",
    "        }\n",
    "        for item in items\n",
    "    ]\n",
    "\n",
    "    if filters.product_type:\n",
    "        items = [item for item in items if item[\"product_type\"] in filters.product_type]\n",
    "\n",
    "    if filters.attributes:\n",
    "        for attr in filters.attributes:\n",
    "            if not attr.values:\n",
    "                continue\n",
    "            curr_items = []\n",
    "            for item in items:\n",
    "                attr_name = attr.name\n",
    "                attr_values = attr.values\n",
    "                item_attr_values = item[\"attributes\"]\n",
    "                for item_attr in item_attr_values:\n",
    "                    if (\n",
    "                        item_attr[\"name\"] == attr_name\n",
    "                        and item_attr[\"value\"] in attr_values\n",
    "                    ):\n",
    "                        curr_items.append(item)\n",
    "                        break\n",
    "\n",
    "            items = curr_items\n",
    "\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>category</th>\n",
       "      <th>product_type</th>\n",
       "      <th>price</th>\n",
       "      <th>attributes</th>\n",
       "      <th>in_stock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>High-Waisted Pencil Skirt</td>\n",
       "      <td>This elegant high-waisted pencil skirt is desi...</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Skirts</td>\n",
       "      <td>64.18</td>\n",
       "      <td>[{'name': 'Rise', 'value': 'High Rise'}, {'nam...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>White Eyelet Mini Skirt</td>\n",
       "      <td>Featuring a delicate eyelet design, this white...</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Skirts</td>\n",
       "      <td>102.31</td>\n",
       "      <td>[{'name': 'Length', 'value': 'Mini'}, {'name':...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Plaid Pencil Skirt</td>\n",
       "      <td>This plaid pencil skirt is a versatile additio...</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Skirts</td>\n",
       "      <td>275.42</td>\n",
       "      <td>[{'name': 'Fit', 'value': 'Straight'}, {'name'...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Black Denim Skirt</td>\n",
       "      <td>A versatile black denim skirt with front pocke...</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Skirts</td>\n",
       "      <td>289.67</td>\n",
       "      <td>[{'name': 'Rise', 'value': 'Mid Rise'}, {'name...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Green Plaid Mini Skirt</td>\n",
       "      <td>Add a pop of pattern to your outfit with this ...</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Skirts</td>\n",
       "      <td>191.17</td>\n",
       "      <td>[{'name': 'Rise', 'value': 'High Rise'}, {'nam...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       title  ... in_stock\n",
       "0  High-Waisted Pencil Skirt  ...     True\n",
       "1    White Eyelet Mini Skirt  ...     True\n",
       "2         Plaid Pencil Skirt  ...    False\n",
       "3          Black Denim Skirt  ...     True\n",
       "4     Green Plaid Mini Skirt  ...     True\n",
       "\n",
       "[5 rows x 7 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_query = \"I want a skirt that is at most 300 bucks\"\n",
    "client = instructor.from_openai(AsyncOpenAI())\n",
    "db = connect(\"./lancedb\")\n",
    "table = db.open_table(\"items\")\n",
    "generated_filter = await extract_query_filters(client, test_query)\n",
    "results = retrieve_and_filter(test_query, table, generated_filter)\n",
    "pd.DataFrame(results).loc[\n",
    "    :,\n",
    "    [\n",
    "        \"title\",\n",
    "        \"description\",\n",
    "        \"category\",\n",
    "        \"product_type\",\n",
    "        \"price\",\n",
    "        \"attributes\",\n",
    "        \"in_stock\",\n",
    "    ],\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the filter is working as expected. Let's now apply this new filtering to our retrieval system and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load in queries that we generated previously\n",
    "with open(\"queries.json\", \"r\") as f:\n",
    "    queries = [json.loads(line) for line in f]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [01:48<00:00,  2.86s/it]\n"
     ]
    }
   ],
   "source": [
    "import instructor\n",
    "from lancedb.table import Table\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "\n",
    "async def retrieve_item(client: instructor.AsyncInstructor, item: dict, table: Table):\n",
    "    generated_filter = await extract_query_filters(client, item[\"query\"])\n",
    "    results = retrieve_and_filter(item[\"query\"], table, generated_filter)\n",
    "    return {\n",
    "        \"query\": item[\"query\"],\n",
    "        \"retrieved_items\": results,\n",
    "        \"expected_items\": [item[\"id\"]],\n",
    "        \"filters\": generated_filter,\n",
    "    }\n",
    "\n",
    "\n",
    "client = instructor.from_openai(AsyncOpenAI())\n",
    "coros = [retrieve_item(client, query, table) for query in queries]\n",
    "results = await tqdm_asyncio.gather(*coros)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now use this to evaluate the performance of our retrieval system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_items = [\n",
    "    [item[\"id\"] for item in result[\"retrieved_items\"]] for result in results\n",
    "]\n",
    "labels = [item[\"expected_items\"] for item in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.metrics import get_metrics_at_k\n",
    "\n",
    "metrics = get_metrics_at_k(metrics=[\"mrr\", \"recall\"], sizes=[5, 10, 25])\n",
    "computed_metrics = [\n",
    "    {\n",
    "        metric: score_fn(retrieved_item_ids, desired_item)\n",
    "        for metric, score_fn in metrics.items()\n",
    "    }\n",
    "    for retrieved_item_ids, desired_item in zip(retrieved_items, labels)\n",
    "]\n",
    "df = pd.DataFrame(computed_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mrr@5        0.500000\n",
       "mrr@10       0.527778\n",
       "mrr@25       0.527778\n",
       "recall@5     0.500000\n",
       "recall@10    0.750000\n",
       "recall@25    0.750000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Braintrust\n",
    "\n",
    "When working with these larger evaluation datasets, it's important to have a way to visualise the results easily. For this, we'll switch over to using Braintrust to visualise these results so that we can iterate on the prompts for our metadata filters better.\n",
    "\n",
    "We'll redefine our `extract_query_filters` function below again and iteratively modify our prompt to get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import instructor\n",
    "\n",
    "\n",
    "async def extract_query_filters(\n",
    "    client: instructor.AsyncInstructor, query: str\n",
    ") -> QueryFilters:\n",
    "    return await client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"\n",
    "You are a helpful assistant that extracts user requirements from a query.\n",
    "\n",
    "Use this following taxonomy as a reference for what fields are available to you. Only use values from the provided taxonomy.\n",
    "\n",
    "<taxonomy>\n",
    "{{ taxonomy_data }}\n",
    "</taxonomy>\n",
    "\n",
    "Guidelines:\n",
    "\n",
    "- If a specific filter isn't needed, just return an empty list or null value for that\n",
    "- If the attribute exists on multiple types, make sure that you only look at the specific types listed under the category you have chosen\n",
    "- Make sure that you've chosen from the right attribute values for each attribute type. This is very important.\n",
    "\n",
    "Here are some general rules about how to generate these filters\n",
    "1. Dresses and Skirts should always go together\n",
    "2. Potential Filters should only be added if the user has explicitly mentioned it. When selecting filter values, aim to make them more flexible. For instance, if the user is asking for a well fitting top, we can consider both regular and relaxed fit. If another attribute value might be a good match for the user's query, include it too. \n",
    "3. If the user hasn't mentioned the attribute for the product type in his query, don't include that attribute in the filter. For instance if the user only mentions that they want something that's comfortable, don't include a filter for the fit of the product.\n",
    "4. If the user mentions a rough range ( eg. around 50 bucks), let's just use a buffer of 30 bucks on each side ( Eg. 20-80)\n",
    "5. If the user mentions a vague price (Eg. I have a high budget), just set max price to 1000\n",
    "6. Make sure to look carefully at the user's query to determine if they've specified a specific fit - eg. regular, relaxed, cropped. ( Relaxed and Relaxed should always go together)\n",
    "\n",
    "\"\"\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": query},\n",
    "        ],\n",
    "        context={\n",
    "            \"taxonomy_data\": taxonomy_data,\n",
    "        },\n",
    "        response_model=QueryFilters,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define our braintrust `AsyncEval` and then use it to iterate on our prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping git metadata. This is likely because the repository has not been published to a remote yet. Remote named 'origin' didn't exist\n",
      "Experiment main-1739390828 is running at https://www.braintrust.dev/app/567/p/query-generation/experiments/main-1739390828\n",
      "query-generation (data): 38it [00:00, 74792.84it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f48e6542403c4e599372b2be8eaace44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "query-generation (tasks):   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ws/q_m6c6qs3n553603dk_zvrgc0000gn/T/ipykernel_7946/737981449.py:32: DeprecationWarning: meta() is deprecated. Use the metadata field directly instead.\n",
      "  hooks.meta(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "main-1739390828 compared to main-1739390737:\n",
      "73.68% (-01.99%) 'mrr@1'     score\t(1 improvements, 1 regressions)\n",
      "78.07% (-02.11%) 'mrr@3'     score\t(1 improvements, 1 regressions)\n",
      "78.07% (-02.11%) 'mrr@5'     score\t(1 improvements, 1 regressions)\n",
      "79.18% (-02.14%) 'mrr@10'    score\t(1 improvements, 1 regressions)\n",
      "79.18% (-02.14%) 'mrr@15'    score\t(1 improvements, 1 regressions)\n",
      "79.18% (-02.14%) 'mrr@25'    score\t(1 improvements, 1 regressions)\n",
      "73.68% (-01.99%) 'recall@1'  score\t(1 improvements, 1 regressions)\n",
      "84.21% (-02.28%) 'recall@3'  score\t(1 improvements, 1 regressions)\n",
      "84.21% (-02.28%) 'recall@5'  score\t(1 improvements, 1 regressions)\n",
      "92.11% (-02.49%) 'recall@10' score\t(1 improvements, 1 regressions)\n",
      "92.11% (-02.49%) 'recall@15' score\t(1 improvements, 1 regressions)\n",
      "92.11% (-02.49%) 'recall@25' score\t(1 improvements, 1 regressions)\n",
      "\n",
      "1739390828.04s start\n",
      "1739390845.33s end\n",
      "11.27s (+287.71%) 'duration'\t(11 improvements, 27 regressions)\n",
      "\n",
      "See results for main-1739390828 at https://www.braintrust.dev/app/567/p/query-generation/experiments/main-1739390828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EvalResultWithSummary(summary=\"...\", results=[...])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from braintrust import EvalAsync, Score\n",
    "from helpers.metrics import get_metrics_at_k\n",
    "import lancedb\n",
    "import openai\n",
    "\n",
    "\n",
    "def evaluate_braintrust(input, output, **kwargs):\n",
    "    metrics = get_metrics_at_k(metrics=[\"mrr\", \"recall\"], sizes=[1, 3, 5, 10, 15, 25])\n",
    "    return [\n",
    "        Score(\n",
    "            name=metric,\n",
    "            score=score_fn(output, kwargs[\"expected\"]),\n",
    "            metadata={\"query\": input, \"result\": output, **kwargs[\"metadata\"]},\n",
    "        )\n",
    "        for metric, score_fn in metrics.items()\n",
    "    ]\n",
    "\n",
    "\n",
    "client = instructor.from_openai(openai.AsyncOpenAI())\n",
    "taxonomy_data = process_taxonomy_file(\"taxonomy.yml\")\n",
    "with open(\"queries.json\", \"r\") as f:\n",
    "    queries = [json.loads(line) for line in f]\n",
    "db = lancedb.connect(\"./lancedb\")\n",
    "table = db.open_table(\"items\")\n",
    "\n",
    "\n",
    "async def generate_filters_and_retrieve_items(query: dict, hooks) -> dict:\n",
    "    generated_filter = await extract_query_filters(client, query[\"query\"])\n",
    "    results = retrieve_and_filter(query[\"query\"], table, generated_filter)\n",
    "    items_without_vector = [\n",
    "        {k: v for k, v in item.items() if k != \"vector\"} for item in results\n",
    "    ]\n",
    "\n",
    "    hooks.meta(filters=generated_filter.model_dump(), items=items_without_vector)\n",
    "    return [item[\"id\"] for item in results]\n",
    "\n",
    "\n",
    "await EvalAsync(\n",
    "    \"query-generation\",\n",
    "    data=lambda: [{\"input\": query, \"expected\": [query[\"id\"]]} for query in queries],\n",
    "    task=generate_filters_and_retrieve_items,\n",
    "    scores=[evaluate_braintrust],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the filters are working as expected. By implementing metadata filters, we've been able to improve the recall and MRR of our retrieval system by a significant margin as seen below.\n",
    "\n",
    "| Metric | Semantic Search (Baseline) | Semantic Search + Metadata Filters |\n",
    "|---------|--------------------------|----------------------------------|\n",
    "| MRR@5   | 0.47 | 0.78 (+65.43%) |\n",
    "| MRR@10  | 0.49 | 0.79 (+61.67%) |\n",
    "| MRR@15  | 0.49 | 0.79 (+61.06%) |\n",
    "| MRR@20  | 0.49 | 0.79 (+60.05%) |\n",
    "| MRR@25  | 0.49 | 0.79 (+60.05%) |\n",
    "| Recall@5  | 0.63 | 0.84 (+33.33%) |\n",
    "| Recall@10 | 0.76 | 0.92 (+20.70%) |\n",
    "| Recall@15 | 0.79 | 0.92 (+16.67%) |\n",
    "| Recall@20 | 0.84 | 0.92 (+9.38%) |\n",
    "| Recall@25 | 0.84 | 0.92 (+9.38%) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most importantly, we've been able to quantify the improvement in our retrieval at each step. This allows us to know the impact of each step in our retrieval pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Better Captioning\n",
    "\n",
    "We can also use the same concept to explore other ways to improve our retrieval system. For instance, we can improve the captioning of our items by providing more information to be embedded. \n",
    "\n",
    "Let's see how we can do this below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[90m[\u001b[0m2025-02-12T20:18:05Z \u001b[33mWARN \u001b[0m lance::dataset::write::insert\u001b[90m]\u001b[0m No existing dataset at /Users/ivanleo/Documents/coding/ai-engineer-summit/lancedb/items_better_captioning.lance, it will be created\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">191</span> rows in the table\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m191\u001b[0m rows in the table\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lancedb.pydantic import LanceModel, Vector\n",
    "import lancedb\n",
    "from lancedb.embeddings import get_registry\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"ivanleomk/ai-engineer-summit-ecommerce-taxonomy\")[\"train\"]\n",
    "func = get_registry().get(\"openai\").create(name=\"text-embedding-3-small\")\n",
    "\n",
    "\n",
    "# Define a Model that will be used as the schema for our collection\n",
    "class Item(LanceModel):\n",
    "    id: int\n",
    "    title: str\n",
    "    description: str = func.SourceField()\n",
    "    brand: str\n",
    "    category: str\n",
    "    product_type: str\n",
    "    attributes: str\n",
    "    material: str\n",
    "    pattern: str\n",
    "    price: float\n",
    "    vector: Vector(func.ndims()) = func.VectorField()\n",
    "    in_stock: bool\n",
    "\n",
    "\n",
    "db = lancedb.connect(\"./lancedb\")\n",
    "table_name = \"items_better_captioning\"\n",
    "\n",
    "if table_name not in db.table_names():\n",
    "    table = db.create_table(table_name, schema=Item, mode=\"overwrite\")\n",
    "    entries = []\n",
    "    for row in dataset:\n",
    "        entries.append(\n",
    "            {\n",
    "                \"id\": row[\"id\"],\n",
    "                \"title\": row[\"title\"],\n",
    "                \"description\": f\"\"\"\n",
    "title: {row[\"title\"]}\n",
    "description: {row[\"description\"]}\n",
    "brand: {row[\"brand\"]}\n",
    "category: {row[\"category\"]}\n",
    "product_type: {row[\"product_type\"]}\n",
    "price: {row[\"price\"]}\n",
    "attributes: {json.dumps(row[\"attributes\"])}\n",
    "\"\"\".strip(),\n",
    "                \"brand\": row[\"brand\"],\n",
    "                \"category\": row[\"category\"],\n",
    "                \"product_type\": row[\"product_type\"],\n",
    "                \"attributes\": row[\"attributes\"],\n",
    "                \"material\": row[\"material\"],\n",
    "                \"pattern\": row[\"pattern\"],\n",
    "                \"price\": row[\"price\"],\n",
    "                \"in_stock\": row[\"in_stock\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    table.add(entries)\n",
    "\n",
    "table = db.open_table(table_name)\n",
    "print(f\"{table.count_rows()} rows in the table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping git metadata. This is likely because the repository has not been published to a remote yet. Remote named 'origin' didn't exist\n",
      "Experiment main-1739391749 is running at https://www.braintrust.dev/app/567/p/query-generation/experiments/main-1739391749\n",
      "query-generation (data): 38it [00:00, 24200.36it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21c26d3a6e8b489599b4fb9c88e137f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "query-generation (tasks):   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "main-1739391749 compared to main-1739391719:\n",
      "50.00% 'mrr@1'     score\n",
      "53.95% 'mrr@3'     score\n",
      "56.18% 'mrr@5'     score\n",
      "58.00% 'mrr@10'    score\n",
      "58.22% 'mrr@15'    score\n",
      "58.48% 'mrr@25'    score\n",
      "50.00% 'recall@1'  score\n",
      "60.53% 'recall@3'  score\n",
      "71.05% 'recall@5'  score\n",
      "84.21% 'recall@10' score\n",
      "86.84% 'recall@15' score\n",
      "92.11% 'recall@25' score\n",
      "\n",
      "1739391749.20s start\n",
      "1739391749.95s end\n",
      "0.60s (+26.48%) 'duration'\t(7 improvements, 31 regressions)\n",
      "\n",
      "See results for main-1739391749 at https://www.braintrust.dev/app/567/p/query-generation/experiments/main-1739391749\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EvalResultWithSummary(summary=\"...\", results=[...])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from braintrust import EvalAsync, Score\n",
    "from helpers.metrics import get_metrics_at_k\n",
    "import lancedb\n",
    "import openai\n",
    "\n",
    "\n",
    "def evaluate_braintrust(input, output, **kwargs):\n",
    "    metrics = get_metrics_at_k(metrics=[\"mrr\", \"recall\"], sizes=[1, 3, 5, 10, 15, 25])\n",
    "    return [\n",
    "        Score(\n",
    "            name=metric,\n",
    "            score=score_fn(output, kwargs[\"expected\"]),\n",
    "            metadata={\"query\": input, \"result\": output, **kwargs[\"metadata\"]},\n",
    "        )\n",
    "        for metric, score_fn in metrics.items()\n",
    "    ]\n",
    "\n",
    "\n",
    "table = db.open_table(\"items_better_captioning\")\n",
    "\n",
    "\n",
    "def task(query: dict, hooks) -> dict:\n",
    "    return [item[\"id\"] for item in table.search(query[\"query\"]).limit(25).to_list()]\n",
    "\n",
    "\n",
    "await EvalAsync(\n",
    "    \"query-generation\",\n",
    "    data=lambda: [{\"input\": query, \"expected\": [query[\"id\"]]} for query in queries],\n",
    "    task=task,\n",
    "    scores=[evaluate_braintrust],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we see an average increase of ~18% for MRR and ~10% for recall across all values of `k` for this new improved description that we're embedding.\n",
    "\n",
    "| Metric | Semantic Search (Baseline) | Semantic Search + Metadata |\n",
    "|---------|--------------------------|----------------------------------|\n",
    "| MRR@5   | 0.47 | 0.56 (+19.15%) |\n",
    "| MRR@10  | 0.49 | 0.58 (+18.43%) |\n",
    "| MRR@15  | 0.49 | 0.58 (+18.42%) |\n",
    "| MRR@20  | 0.49 | 0.58 (+18.21%) |\n",
    "| MRR@25  | 0.49 | 0.58 (+18.21%) |\n",
    "| Recall@5  | 0.63 | 0.71 (+12.50%) |\n",
    "| Recall@10 | 0.76 | 0.84 (+10.34%) |\n",
    "| Recall@15 | 0.79 | 0.87 (+10.00%) |\n",
    "| Recall@20 | 0.84 | 0.92 (+9.52%) |\n",
    "| Recall@25 | 0.84 | 0.92 (+9.52%) |\n",
    "\n",
    "This combined with our original metadata filtering approach would significantly improve the results that we would get. Most importantly, with these synthetic datasets that we've created, we're able to know and quantify the impact at every step of the way. This is huge in helping us to really systematically improve our RAG application "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
