{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3 : Metadata-Enhanced Retrieval\n",
    "\n",
    "> **Note** : This notebook is a preview of [Systematically Improving Your RAG Application](https://maven.com/applied-llms/rag-playbook) which shows you how to turns RAG from a risky experiment into a structured, data-driven practice. You'll learn how to pinpoint what's working, diagnose what's not, and steadily raise the bar on performance and user satisfaction. \n",
    "> \n",
    "> For a preview of the course, please check out [improvingrag.com](https://improvingrag.com) which provides a free preview of the course material.\n",
    "\n",
    "In the previous notebook, we established a performance baseline for our retrieval system using synthetic queries and key metrics like Recall and Mean Reciprocal Rank (MRR). In this notebook, we build on that foundation by incorporating metadata filtering to further enhance retrieval precision and relevance.\n",
    "\n",
    "\n",
    "## Why this matters\n",
    "\n",
    "Even a robust retrieval system can benefit from additional context. By leveraging structured metadata—such as product categories, types, and attributes — we can:\n",
    "\n",
    "1. Improve Relevance: Narrow down search results to closely match the user's intent.\n",
    "2. Enhance Precision: Filter out extraneous items, ensuring that only the most pertinent information is returned.\n",
    "3. Quantify Impact: Use measurable metrics to validate improvements and guide further refinements.\n",
    "\n",
    "## What you'll learn\n",
    "\n",
    "In this notebook, you will learn to:\n",
    "\n",
    "1. Implement Metadata Filtering\n",
    "   - Map user queries to structured metadata using a predefined taxonomy.\n",
    "   - Validate and apply filters to refine the set of candidate items for retrieval.\n",
    "\n",
    "2. Quantify Retrieval Improvements\n",
    "   - Evaluate the impact of metadata filtering on performance using key metrics (Recall and MRR).\n",
    "   - Compare baseline results with enhanced outcomes to understand measurable gains.\n",
    "\n",
    "3. Enhance Item Descriptions with Metadata\n",
    "   - Integrate rich metadata into item descriptions for better semantic matching.\n",
    "   - Leverage enriched data to drive more accurate and relevant retrieval results.\n",
    "\n",
    "By the end of this notebook, you'll have a clear, data-driven approach to elevating your retrieval system—transforming it from a simple search mechanism into a finely tuned, precision tool that consistently delivers improved performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Metadata Fields for filtering\n",
    "\n",
    "For us to use metadata fields for filtering, we need to be able to map a user query to a set of metadata fields that our database items have been annotated with. \n",
    "\n",
    "To make things easier, we'll be using the `instructor` library which provides structured outputs from LLM responses. It also makes things easy to use with in-built jinja support, allowing us to use the same values for validating the generated filters and formatting our prompt itself.\n",
    "\n",
    "This is often known as a taxonomy and we've defined a `taxonomy.yml` ahead of time that defines the metdata fields which we've used to annotate our dataset that we previously ingested. \n",
    "\n",
    "Let's see what this taxonomy looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.taxonomy import process_taxonomy_file\n",
    "\n",
    "taxonomy_map = process_taxonomy_file(\"./taxonomy.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Tops', 'Bottoms', 'Dresses', 'Outerwear'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxonomy_map.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'product_type'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'T-Shirts'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Blouses'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Sweaters'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Cardigans'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Tank Tops'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Hoodies'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Sweatshirts'</span><span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'attributes'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'Sleeve Length'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Sleeveless'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Short Sleeve'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'3/4 Sleeve'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Long Sleeve'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'Neckline'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Crew Neck'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'V-Neck'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Turtleneck'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Scoop Neck'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Cowl Neck'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'Fit'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Regular'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Slim'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Oversized'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Cropped'</span><span style=\"font-weight: bold\">]</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'product_type'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'T-Shirts'\u001b[0m, \u001b[32m'Blouses'\u001b[0m, \u001b[32m'Sweaters'\u001b[0m, \u001b[32m'Cardigans'\u001b[0m, \u001b[32m'Tank Tops'\u001b[0m, \u001b[32m'Hoodies'\u001b[0m, \u001b[32m'Sweatshirts'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'attributes'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'Sleeve Length'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'Sleeveless'\u001b[0m, \u001b[32m'Short Sleeve'\u001b[0m, \u001b[32m'3/4 Sleeve'\u001b[0m, \u001b[32m'Long Sleeve'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[32m'Neckline'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'Crew Neck'\u001b[0m, \u001b[32m'V-Neck'\u001b[0m, \u001b[32m'Turtleneck'\u001b[0m, \u001b[32m'Scoop Neck'\u001b[0m, \u001b[32m'Cowl Neck'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[32m'Fit'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'Regular'\u001b[0m, \u001b[32m'Slim'\u001b[0m, \u001b[32m'Oversized'\u001b[0m, \u001b[32m'Cropped'\u001b[0m\u001b[1m]\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich import print\n",
    "\n",
    "print(taxonomy_map[\"Tops\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Our Response Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for each individual category - `Tops`, `Bottoms`, `Dresses`, `Skirts` etc, we have a set of metadata fields that we can use to filter our items.\n",
    "\n",
    "Defining these in a `.yaml` file is a flexible way to leverage the expertise of domain experts to define these metdata fields. We can then read these fields and values in at run-time and them to generate metadata filters that we can then apply on our retrieval system.\n",
    "\n",
    "In the code below, we've defined a `QueryFilters` model that we'll use to generate metadata filters from a user query, ensuring that we conform to the taxonomy we've defined with the aid of a `field_validator` that we'll use to check the extracted metadata filters. \n",
    "\n",
    "Notice here how we're using the `info:ValidationInfo` context to pass in the taxonomy data to the LLM when we're generating the metadata filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from pydantic import BaseModel, model_validator, ValidationInfo\n",
    "\n",
    "\n",
    "class Attribute(BaseModel):\n",
    "    name: str\n",
    "    values: list[str]\n",
    "\n",
    "\n",
    "class QueryFilters(BaseModel):\n",
    "    attributes: list[Attribute]\n",
    "    min_price: Optional[float] = None\n",
    "    max_price: Optional[float] = None\n",
    "    category: str\n",
    "    product_type: list[str]\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate_attributes(self, info: ValidationInfo):\n",
    "        taxonomy_data = info.context[\"taxonomy_data\"]\n",
    "        # Validate category exists in taxonomy\n",
    "        if self.category not in taxonomy_data:\n",
    "            raise ValueError(\n",
    "                f\"Invalid category: {self.category}. Valid categories are {taxonomy_data.keys()}\"\n",
    "            )\n",
    "\n",
    "        # Validate product types\n",
    "        valid_types = taxonomy_data[self.category][\"product_type\"]\n",
    "        for product_type in self.product_type:\n",
    "            if product_type not in valid_types:\n",
    "                raise ValueError(\n",
    "                    f\"Invalid product type: {product_type}. Valid product types are {valid_types}\"\n",
    "                )\n",
    "\n",
    "        # Validate attribute exists in taxonomy\n",
    "        valid_attrs = taxonomy_data[self.category][\"attributes\"]\n",
    "        for attr in self.attributes:\n",
    "            if attr.name not in valid_attrs:\n",
    "                raise ValueError(f\"Invalid attribute name: {attr.name}\")\n",
    "            for value in attr.values:\n",
    "                if value not in valid_attrs[attr.name]:\n",
    "                    raise ValueError(\n",
    "                        f\"Invalid value {value} for attribute {attr.name}. Valid values are {valid_attrs[attr.name]}\"\n",
    "                    )\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Understanding\n",
    "\n",
    "Now that we've defined our response model, we can use it to generate a metadata filter for our user queries. We'll see an example below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QueryFilters</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">attributes</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Attribute</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Sleeve Length'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">values</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Sleeveless'</span><span style=\"font-weight: bold\">])]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">min_price</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">max_price</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100.0</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">category</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Tops'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">product_type</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Tank Tops'</span><span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mQueryFilters\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mattributes\u001b[0m=\u001b[1m[\u001b[0m\u001b[1;35mAttribute\u001b[0m\u001b[1m(\u001b[0m\u001b[33mname\u001b[0m=\u001b[32m'Sleeve Length'\u001b[0m, \u001b[33mvalues\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'Sleeveless'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[33mmin_price\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mmax_price\u001b[0m=\u001b[1;36m100\u001b[0m\u001b[1;36m.0\u001b[0m,\n",
       "    \u001b[33mcategory\u001b[0m=\u001b[32m'Tops'\u001b[0m,\n",
       "    \u001b[33mproduct_type\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'Tank Tops'\u001b[0m\u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import instructor\n",
    "from helpers.taxonomy import process_taxonomy_file\n",
    "from rich import print\n",
    "\n",
    "# Wrap the OpenAI client in the relevant instructor method\n",
    "client = instructor.from_openai(OpenAI())\n",
    "\n",
    "# Import in our taxonomy data\n",
    "taxonomy_data = process_taxonomy_file(\"taxonomy.yml\")\n",
    "\n",
    "\n",
    "query = \"I want a Tank-Top that's got a short sleeve or sleeveless which is under 100 bucks for an interview\"\n",
    "\n",
    "\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"\n",
    "          You are a helpful assistant that extracts user requirements from a query.\n",
    "                    \n",
    "Use these references:\n",
    "- Taxonomy: {{ taxonomy }}\n",
    "\n",
    "Guidelines:\n",
    "- If a filter isn't needed, return an empty list\n",
    "- Only add attributes and filters that a user has mentioned explicitly\n",
    "- Only use values from the provided taxonomy. \n",
    "- If the attribute exists on multiple types, make sure that you only look at the specific types listed under the category you have chosen\n",
    "- If the user hasn't mentioned a specific product type, lets just use all of them\n",
    "- if the user gives a range (Eg. around 50), just give a buffer of 20 on each side (Eg. 30-70)\n",
    "- if the user gives a vague price (Eg. I have a high budget), just set max price to 1000\n",
    "- only classify an item as unisex if the user has explicitly mentioned it and default to Women's categories by default.\n",
    "- If you're looking at blouses, make sure to include tank tops along the way and vice versa\n",
    "- if the user mentions user bottoms and doesn't specify a specific length - let's include both short and long bottoms such as jeans, shorts and pants\n",
    "- make sure to look carefully at the user's query to determine if they've specified a specific fit - eg. regular, relaxed, cropped. ( Relaxed and Relaxed should always go together)\n",
    "\n",
    "\n",
    "Extract the requirements and format them according to the QueryFilters model.\n",
    "            \"\"\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "    ],\n",
    "    context={\n",
    "        \"taxonomy_data\": taxonomy_data,\n",
    "    },\n",
    "    response_model=QueryFilters,\n",
    ")\n",
    "\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that given the user query, the LLM has generated a set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QueryFilters</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">attributes</span>=<span style=\"font-weight: bold\">[]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">min_price</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">max_price</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">40.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">category</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Bottoms'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">product_type</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Skirts'</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mQueryFilters\u001b[0m\u001b[1m(\u001b[0m\u001b[33mattributes\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mmin_price\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33mmax_price\u001b[0m=\u001b[1;36m40\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33mcategory\u001b[0m=\u001b[32m'Bottoms'\u001b[0m, \u001b[33mproduct_type\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'Skirts'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import instructor\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "\n",
    "async def extract_query_filters(\n",
    "    client: instructor.AsyncInstructor, query: str\n",
    ") -> QueryFilters:\n",
    "    \"\"\"\n",
    "    Extract structured filters from a natural language query using LLM.\n",
    "\n",
    "    Args:\n",
    "        query (str): Natural language query from user\n",
    "\n",
    "    Returns:\n",
    "        QueryFilters: Structured filters extracted from the query\n",
    "    \"\"\"\n",
    "    return await client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant that extracts user requirements from a query. Refer to this taxonomy for valid categories, subcategories, product types and attributes: {{ taxonomy_data }}. If a filter isn't needed, just return an empty list. If the user is looking for a specific attribute, just return the attribute name and the values that the user is looking for\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": query},\n",
    "        ],\n",
    "        context={\n",
    "            \"taxonomy_data\": taxonomy_data,\n",
    "        },\n",
    "        response_model=QueryFilters,\n",
    "    )\n",
    "\n",
    "\n",
    "client = instructor.from_openai(AsyncOpenAI())\n",
    "print(await extract_query_filters(client, \"Need a new skirt around 40 bucks\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see how we can use this metadata filtering with LanceDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from lancedb import connect\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def retrieve_and_filter(query: str, table, filters: QueryFilters, max_k=75):\n",
    "    query_parts = []\n",
    "\n",
    "    # We do a prefilter on category,price and material since these will always be provided\n",
    "    query_parts.append(f\"category='{filters.category}'\")\n",
    "\n",
    "    if filters.min_price:\n",
    "        query_parts.append(f\"price >= {filters.min_price}\")\n",
    "    if filters.max_price:\n",
    "        query_parts.append(f\"price <= {filters.max_price}\")\n",
    "\n",
    "    query_string = \" AND \".join(query_parts)\n",
    "    items = (\n",
    "        table.search(query=query)\n",
    "        .where(query_string, prefilter=True)\n",
    "        .limit(max_k)\n",
    "        .to_list()\n",
    "    )\n",
    "\n",
    "    items = [\n",
    "        {\n",
    "            **item,\n",
    "            \"attributes\": json.loads(item[\"attributes\"]),\n",
    "        }\n",
    "        for item in items\n",
    "    ]\n",
    "\n",
    "    if filters.product_type:\n",
    "        items = [item for item in items if item[\"product_type\"] in filters.product_type]\n",
    "\n",
    "    if filters.attributes:\n",
    "        for attr in filters.attributes:\n",
    "            if not attr.values:\n",
    "                continue\n",
    "            curr_items = []\n",
    "            for item in items:\n",
    "                attr_name = attr.name\n",
    "                attr_values = attr.values\n",
    "                item_attr_values = item[\"attributes\"]\n",
    "                for item_attr in item_attr_values:\n",
    "                    if (\n",
    "                        item_attr[\"name\"] == attr_name\n",
    "                        and item_attr[\"value\"] in attr_values\n",
    "                    ):\n",
    "                        curr_items.append(item)\n",
    "                        break\n",
    "\n",
    "            items = curr_items\n",
    "\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>category</th>\n",
       "      <th>product_type</th>\n",
       "      <th>price</th>\n",
       "      <th>attributes</th>\n",
       "      <th>in_stock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>High-Waisted Pencil Skirt</td>\n",
       "      <td>This elegant high-waisted pencil skirt is desi...</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Skirts</td>\n",
       "      <td>64.18</td>\n",
       "      <td>[{'name': 'Rise', 'value': 'High Rise'}, {'nam...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>White Eyelet Mini Skirt</td>\n",
       "      <td>Featuring a delicate eyelet design, this white...</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Skirts</td>\n",
       "      <td>102.31</td>\n",
       "      <td>[{'name': 'Length', 'value': 'Mini'}, {'name':...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Plaid Pencil Skirt</td>\n",
       "      <td>This plaid pencil skirt is a versatile additio...</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Skirts</td>\n",
       "      <td>275.42</td>\n",
       "      <td>[{'name': 'Fit', 'value': 'Straight'}, {'name'...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Black Denim Skirt</td>\n",
       "      <td>A versatile black denim skirt with front pocke...</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Skirts</td>\n",
       "      <td>289.67</td>\n",
       "      <td>[{'name': 'Rise', 'value': 'Mid Rise'}, {'name...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Green Plaid Mini Skirt</td>\n",
       "      <td>Add a pop of pattern to your outfit with this ...</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Skirts</td>\n",
       "      <td>191.17</td>\n",
       "      <td>[{'name': 'Rise', 'value': 'High Rise'}, {'nam...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       title  ... in_stock\n",
       "0  High-Waisted Pencil Skirt  ...     True\n",
       "1    White Eyelet Mini Skirt  ...     True\n",
       "2         Plaid Pencil Skirt  ...    False\n",
       "3          Black Denim Skirt  ...     True\n",
       "4     Green Plaid Mini Skirt  ...     True\n",
       "\n",
       "[5 rows x 7 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_query = \"I want a skirt that is at most 300 bucks\"\n",
    "client = instructor.from_openai(AsyncOpenAI())\n",
    "db = connect(\"./lancedb\")\n",
    "table = db.open_table(\"items\")\n",
    "generated_filter = await extract_query_filters(client, test_query)\n",
    "results = retrieve_and_filter(test_query, table, generated_filter)\n",
    "pd.DataFrame(results).loc[\n",
    "    :,\n",
    "    [\n",
    "        \"title\",\n",
    "        \"description\",\n",
    "        \"category\",\n",
    "        \"product_type\",\n",
    "        \"price\",\n",
    "        \"attributes\",\n",
    "        \"in_stock\",\n",
    "    ],\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the filter is working as expected. Let's now apply this new filtering to our retrieval system and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load in queries that we generated previously\n",
    "with open(\"queries.json\", \"r\") as f:\n",
    "    queries = [json.loads(line) for line in f]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [01:48<00:00,  2.86s/it]\n"
     ]
    }
   ],
   "source": [
    "import instructor\n",
    "from lancedb.table import Table\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "\n",
    "async def retrieve_item(client: instructor.AsyncInstructor, item: dict, table: Table):\n",
    "    generated_filter = await extract_query_filters(client, item[\"query\"])\n",
    "    results = retrieve_and_filter(item[\"query\"], table, generated_filter)\n",
    "    return {\n",
    "        \"query\": item[\"query\"],\n",
    "        \"retrieved_items\": results,\n",
    "        \"expected_items\": [item[\"id\"]],\n",
    "        \"filters\": generated_filter,\n",
    "    }\n",
    "\n",
    "\n",
    "client = instructor.from_openai(AsyncOpenAI())\n",
    "coros = [retrieve_item(client, query, table) for query in queries]\n",
    "results = await tqdm_asyncio.gather(*coros)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now use this to evaluate the performance of our retrieval system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_items = [\n",
    "    [item[\"id\"] for item in result[\"retrieved_items\"]] for result in results\n",
    "]\n",
    "labels = [item[\"expected_items\"] for item in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.metrics import get_metrics_at_k\n",
    "\n",
    "metrics = get_metrics_at_k(metrics=[\"mrr\", \"recall\"], sizes=[5, 10, 25])\n",
    "computed_metrics = [\n",
    "    {\n",
    "        metric: score_fn(retrieved_item_ids, desired_item)\n",
    "        for metric, score_fn in metrics.items()\n",
    "    }\n",
    "    for retrieved_item_ids, desired_item in zip(retrieved_items, labels)\n",
    "]\n",
    "df = pd.DataFrame(computed_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mrr@5        0.500000\n",
       "mrr@10       0.527778\n",
       "mrr@25       0.527778\n",
       "recall@5     0.500000\n",
       "recall@10    0.750000\n",
       "recall@25    0.750000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Braintrust\n",
    "\n",
    "When working with these larger evaluation datasets, it's important to have a way to visualise the results easily. For this, we'll switch over to using Braintrust to visualise these results so that we can iterate on the prompts for our metadata filters better.\n",
    "\n",
    "We'll redefine our `extract_query_filters` function below again and iteratively modify our prompt to get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import instructor\n",
    "\n",
    "\n",
    "async def extract_query_filters(\n",
    "    client: instructor.AsyncInstructor, query: str\n",
    ") -> QueryFilters:\n",
    "    return await client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"\n",
    "You are a helpful assistant that extracts user requirements from a query.\n",
    "\n",
    "Use this following taxonomy as a reference for what fields are available to you. Only use values from the provided taxonomy.\n",
    "\n",
    "<taxonomy>\n",
    "{{ taxonomy_data }}\n",
    "</taxonomy>\n",
    "\n",
    "Guidelines:\n",
    "\n",
    "- If a specific filter isn't needed, just return an empty list or null value for that\n",
    "- If the attribute exists on multiple types, make sure that you only look at the specific types listed under the category you have chosen\n",
    "- Make sure that you've chosen from the right attribute values for each attribute type. This is very important.\n",
    "\n",
    "Here are some general rules about how to generate these filters\n",
    "1. Dresses and Skirts should always go together\n",
    "2. Potential Filters should only be added if the user has explicitly mentioned it. When selecting filter values, aim to make them more flexible. For instance, if the user is asking for a well fitting top, we can consider both regular and relaxed fit. If another attribute value might be a good match for the user's query, include it too. \n",
    "3. If the user hasn't mentioned the attribute for the product type in his query, don't include that attribute in the filter. For instance if the user only mentions that they want something that's comfortable, don't include a filter for the fit of the product.\n",
    "4. If the user mentions a rough range ( eg. around 50 bucks), let's just use a buffer of 30 bucks on each side ( Eg. 20-80)\n",
    "5. If the user mentions a vague price (Eg. I have a high budget), just set max price to 1000\n",
    "6. Make sure to look carefully at the user's query to determine if they've specified a specific fit - eg. regular, relaxed, cropped. ( Relaxed and Relaxed should always go together)\n",
    "\n",
    "\"\"\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": query},\n",
    "        ],\n",
    "        context={\n",
    "            \"taxonomy_data\": taxonomy_data,\n",
    "        },\n",
    "        response_model=QueryFilters,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define our braintrust `AsyncEval` and then use it to iterate on our prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping git metadata. This is likely because the repository has not been published to a remote yet. Remote named 'origin' didn't exist\n",
      "Experiment main-1739390828 is running at https://www.braintrust.dev/app/567/p/query-generation/experiments/main-1739390828\n",
      "query-generation (data): 38it [00:00, 74792.84it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f48e6542403c4e599372b2be8eaace44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "query-generation (tasks):   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ws/q_m6c6qs3n553603dk_zvrgc0000gn/T/ipykernel_7946/737981449.py:32: DeprecationWarning: meta() is deprecated. Use the metadata field directly instead.\n",
      "  hooks.meta(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "main-1739390828 compared to main-1739390737:\n",
      "73.68% (-01.99%) 'mrr@1'     score\t(1 improvements, 1 regressions)\n",
      "78.07% (-02.11%) 'mrr@3'     score\t(1 improvements, 1 regressions)\n",
      "78.07% (-02.11%) 'mrr@5'     score\t(1 improvements, 1 regressions)\n",
      "79.18% (-02.14%) 'mrr@10'    score\t(1 improvements, 1 regressions)\n",
      "79.18% (-02.14%) 'mrr@15'    score\t(1 improvements, 1 regressions)\n",
      "79.18% (-02.14%) 'mrr@25'    score\t(1 improvements, 1 regressions)\n",
      "73.68% (-01.99%) 'recall@1'  score\t(1 improvements, 1 regressions)\n",
      "84.21% (-02.28%) 'recall@3'  score\t(1 improvements, 1 regressions)\n",
      "84.21% (-02.28%) 'recall@5'  score\t(1 improvements, 1 regressions)\n",
      "92.11% (-02.49%) 'recall@10' score\t(1 improvements, 1 regressions)\n",
      "92.11% (-02.49%) 'recall@15' score\t(1 improvements, 1 regressions)\n",
      "92.11% (-02.49%) 'recall@25' score\t(1 improvements, 1 regressions)\n",
      "\n",
      "1739390828.04s start\n",
      "1739390845.33s end\n",
      "11.27s (+287.71%) 'duration'\t(11 improvements, 27 regressions)\n",
      "\n",
      "See results for main-1739390828 at https://www.braintrust.dev/app/567/p/query-generation/experiments/main-1739390828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EvalResultWithSummary(summary=\"...\", results=[...])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from braintrust import EvalAsync, Score\n",
    "from helpers.metrics import get_metrics_at_k\n",
    "import lancedb\n",
    "import openai\n",
    "\n",
    "\n",
    "def evaluate_braintrust(input, output, **kwargs):\n",
    "    metrics = get_metrics_at_k(metrics=[\"mrr\", \"recall\"], sizes=[1, 3, 5, 10, 15, 25])\n",
    "    return [\n",
    "        Score(\n",
    "            name=metric,\n",
    "            score=score_fn(output, kwargs[\"expected\"]),\n",
    "            metadata={\"query\": input, \"result\": output, **kwargs[\"metadata\"]},\n",
    "        )\n",
    "        for metric, score_fn in metrics.items()\n",
    "    ]\n",
    "\n",
    "\n",
    "client = instructor.from_openai(openai.AsyncOpenAI())\n",
    "taxonomy_data = process_taxonomy_file(\"taxonomy.yml\")\n",
    "with open(\"queries.json\", \"r\") as f:\n",
    "    queries = [json.loads(line) for line in f]\n",
    "db = lancedb.connect(\"./lancedb\")\n",
    "table = db.open_table(\"items\")\n",
    "\n",
    "\n",
    "async def generate_filters_and_retrieve_items(query: dict, hooks) -> dict:\n",
    "    generated_filter = await extract_query_filters(client, query[\"query\"])\n",
    "    results = retrieve_and_filter(query[\"query\"], table, generated_filter)\n",
    "    items_without_vector = [\n",
    "        {k: v for k, v in item.items() if k != \"vector\"} for item in results\n",
    "    ]\n",
    "\n",
    "    hooks.meta(filters=generated_filter.model_dump(), items=items_without_vector)\n",
    "    return [item[\"id\"] for item in results]\n",
    "\n",
    "\n",
    "await EvalAsync(\n",
    "    \"query-generation\",\n",
    "    data=lambda: [{\"input\": query, \"expected\": [query[\"id\"]]} for query in queries],\n",
    "    task=generate_filters_and_retrieve_items,\n",
    "    scores=[evaluate_braintrust],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the filters are working as expected. By implementing metadata filters, we've been able to improve the recall and MRR of our retrieval system by a significant margin as seen below.\n",
    "\n",
    "| Metric | Semantic Search (Baseline) | Semantic Search + Metadata Filters |\n",
    "|---------|--------------------------|----------------------------------|\n",
    "| MRR@5   | 0.47 | 0.78 (+65.43%) |\n",
    "| MRR@10  | 0.49 | 0.79 (+61.67%) |\n",
    "| MRR@15  | 0.49 | 0.79 (+61.06%) |\n",
    "| MRR@20  | 0.49 | 0.79 (+60.05%) |\n",
    "| MRR@25  | 0.49 | 0.79 (+60.05%) |\n",
    "| Recall@5  | 0.63 | 0.84 (+33.33%) |\n",
    "| Recall@10 | 0.76 | 0.92 (+20.70%) |\n",
    "| Recall@15 | 0.79 | 0.92 (+16.67%) |\n",
    "| Recall@20 | 0.84 | 0.92 (+9.38%) |\n",
    "| Recall@25 | 0.84 | 0.92 (+9.38%) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most importantly, we've been able to quantify the improvement in our retrieval at each step. This allows us to know the impact of each step in our retrieval pipeline. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
