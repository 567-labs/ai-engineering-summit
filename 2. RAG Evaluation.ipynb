{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: Evaluating Your RAG Application\n",
    "\n",
    "> **Note** : This notebook is a preview of what we cover in [improvingrag.com](https://improvingrag.com). Stop settling for \"Looks Good to Me\" and start building better systems. Learn how to turn RAG from a risky experiment into a structured, data-driven practice. Check out [improvingrag.com](https://improvingrag.com) for a proven foundational framework to help you go beyond the basics to improve performance, quality, and user experience. \n",
    "\n",
    "\n",
    "In [Notebook 1: Systematically Improving Your RAG Application](#notebook-1-systematically-improving-your-rag-application) we saw why focusing solely on generation can be both costly and slow. In this notebook, we'll shift our focus over to the retrieval side of things, showing how we can benchmark the performance of our retrieval system and improve it over time.\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "Without accurate and relevant retrieval, even the most advanced language models struggle to generate useful answers. By establishing objective performance baselines using synthetic evaluation datasets and key retrieval metrics, you can:\n",
    "- **Diagnose Issues:** Identify gaps where relevant information is missed.\n",
    "- **Measure Effectiveness:** Use quantitative metrics (e.g., Recall, Mean Reciprocal Rank) to assess and compare retrieval performance.\n",
    "- **Iterate Rapidly:** Drive targeted, data-driven improvements to your retrieval pipeline, ultimately enhancing overall user satisfaction.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "In this notebook, you will learn to:\n",
    "\n",
    "1. **Generate Synthetic Evaluation Datasets**\n",
    "   - Leverage language models to create diverse synthetic queries.\n",
    "   - Introduce controlled variations to mimic real-world user inputs.\n",
    "   - Continuously refine these datasets with actual user data.\n",
    "\n",
    "2. **Define and Compute Key Retrieval Metrics**\n",
    "   - Understand metrics such as Precision, Recall, and Mean Reciprocal Rank (MRR).\n",
    "   - Evaluate retrieval performance at multiple cutoff points (e.g., top-5, top-10).\n",
    "   - Connect these metrics to real-world impacts on user experience.\n",
    "\n",
    "3. **Establish a Baseline for Your Retrieval System**\n",
    "   - Assess single-query performance and aggregate results across a dataset.\n",
    "   - Benchmark improvements from enhancements like better metadata captioning.\n",
    "   - Use data-driven insights to guide systematic, iterative improvements in your RAG application.\n",
    "\n",
    "By the end of this notebook, you’ll have a solid framework for measuring and improving the retrieval component of your RAG system, setting the stage for faster, more effective iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Datasets\n",
    "\n",
    "By leveraging language models, we can bootstrap evaluation datasets with synthetic data. This allows us to understand specific areas that our retrieval might suffer before we ship to production. This is a process that should continue, even after we ship to production. We can continously iterate on these synthetic datasets to make sure that they're representative of the queries we can expect in production.\n",
    "\n",
    "Ultimately, this allows us to iteratively update our synthetic datasets over time and make sure that they're representative of the queries we can expect in production. In this section, we'll examine this in 3 parts\n",
    "\n",
    "1. First we'll talk briefly about what Synthetic Data is\n",
    "2. Then we'll look at why we need to be mindful about the diversity in our synthetic data and how introducing controlled variation into the process can help us generate a more representative set of queries\n",
    "3. Then we'll look at how we can generate an initial set of synthetic questions using the items in our dataset for reference to benchmark our retrieval system\n",
    "\n",
    "At the end of this section you should have a clear idea of what synthetic data is and how you can generate your own variations of it.\n",
    "\n",
    "### What is Synthetic Data?\n",
    "\n",
    "Synthetic data is data that's not generated by a human. In our specific context here, we're refering to data that's generated by a language model itself.\n",
    "\n",
    "By leveraging language models to generate questions for us and thinking carefully about the constraints we want to apply to these questions, we can generate datasets that are much richer and diverse than what we could do ourselves. This is because of the sheer amount of data that's been used to train these language models in the first place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Question: What is the estimated population of Paris?\n",
      "Answer: The estimated population of Paris is 2.1 million residents.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import instructor\n",
    "from pydantic import BaseModel\n",
    "\n",
    "client = instructor.from_openai(OpenAI())\n",
    "\n",
    "\n",
    "class Question(BaseModel):\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Sample information about Paris\n",
    "paris_info = \"\"\"\n",
    "Paris is the capital and largest city of France, with an estimated population of 2.1 million residents.\n",
    "Located in northern France, it is a major global center for art, fashion, gastronomy and culture.\n",
    "The city is known for its iconic landmarks including:\n",
    "- The Eiffel Tower\n",
    "- The Louvre Museum\n",
    "- Notre-Dame Cathedral\n",
    "- Arc de Triomphe\n",
    "Paris is also home to world-class universities, financial institutions, and is one of the world's leading tourist destinations.\n",
    "\"\"\"\n",
    "\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant that can generate detailed questions and answers based on the information provided below. Include specific facts and figures where relevant.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": paris_info},\n",
    "    ],\n",
    "    response_model=Question,\n",
    ")\n",
    "\n",
    "print(f\"Generated Question: {resp.question}\")\n",
    "print(f\"Answer: {resp.answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we've generated our first synthetic question! We can see how by scaling this process out, we can generate a large amount of questions easily and efficiently. \n",
    "\n",
    "However, this doesn't come without some challenges - the biggest of which is that of diversity which we'll talk about in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Diversity Matters\n",
    "\n",
    "As mentioned above, the biggest challenge when generating synthetic data is that of diversity. If we pass the language model the same prompt over and over again, we'll get the same output every time. \n",
    "\n",
    "Let's see this in action below when we use the prompt above to generate 4 questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Question: What is the capital and largest city of France, and what is its estimated population?\n",
      "Generated Question: What is the capital and largest city of France?\n",
      "Generated Question: What is the capital and largest city of France, and what is its estimated population?\n",
      "Generated Question: What is the capital and largest city of France?\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import instructor\n",
    "from pydantic import BaseModel\n",
    "\n",
    "client = instructor.from_openai(OpenAI())\n",
    "\n",
    "\n",
    "class Question(BaseModel):\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Sample information about Paris\n",
    "paris_info = \"\"\"\n",
    "Paris is the capital and largest city of France, with an estimated population of 2.1 million residents.\n",
    "Located in northern France, it is a major global center for art, fashion, gastronomy and culture.\n",
    "The city is known for its iconic landmarks including:\n",
    "- The Eiffel Tower\n",
    "- The Louvre Museum\n",
    "- Notre-Dame Cathedral\n",
    "- Arc de Triomphe\n",
    "Paris is also home to world-class universities, financial institutions, and is one of the world's leading tourist destinations.\n",
    "\"\"\"\n",
    "\n",
    "for _ in range(4):\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant that can generate detailed questions and answers based on the information provided below.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"\n",
    "             content: { paris_info }\n",
    "             \"\"\"},\n",
    "        ],\n",
    "        response_model=Question,\n",
    "    )\n",
    "\n",
    "    print(f\"Generated Question: {resp.question}\")\n",
    "    # print(f\"Answer: {resp.answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that given the same prompt, the language model will generate very similar outputs over and over again. In order to combat this, we need to introduce some controlled variation into the process.\n",
    "\n",
    "Now this needs to be done in a way that makes sense. Just using random bits of information in the prompt won't introduce the variation you need to get a diverse set of questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Question: What is the estimated population of Paris?\n",
      "Generated Question: What is the population of Paris as of the latest estimates?\n",
      "Generated Question: What is the estimated population of Paris and what is its significance in the world?\n",
      "Generated Question: What is the capital and largest city of France, and what is its estimated population?\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import instructor\n",
    "from pydantic import BaseModel\n",
    "import random\n",
    "\n",
    "client = instructor.from_openai(OpenAI())\n",
    "\n",
    "\n",
    "class Question(BaseModel):\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Sample information about Paris\n",
    "paris_info = \"\"\"\n",
    "Paris is the capital and largest city of France, with an estimated population of 2.1 million residents.\n",
    "Located in northern France, it is a major global center for art, fashion, gastronomy and culture.\n",
    "The city is known for its iconic landmarks including:\n",
    "- The Eiffel Tower\n",
    "- The Louvre Museum\n",
    "- Notre-Dame Cathedral\n",
    "- Arc de Triomphe\n",
    "Paris is also home to world-class universities, financial institutions, and is one of the world's leading tourist destinations.\n",
    "\"\"\"\n",
    "\n",
    "for _ in range(4):\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"You are a helpful assistant that can generate detailed questions and answers based on the information provided below. Include specific facts and figures where relevant. Note that the current time is {random.randint(1, 12)}:{random.randint(0, 59)}\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"\n",
    "             content: { paris_info }\n",
    "             \"\"\"},\n",
    "        ],\n",
    "        response_model=Question,\n",
    "    )\n",
    "\n",
    "    print(f\"Generated Question: {resp.question}\")\n",
    "    # print(f\"Answer: {resp.answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply introducing variation in the prompt itself was not enough to get a diverse set of questions. In this specific case, we had two unique questions, with the rest being the same.\n",
    "\n",
    "Instead, what we need to do is to introduce smart modes of variation.\n",
    "\n",
    "For instance, in this case here, we could do the following\n",
    "\n",
    "- We could vary the tone that we use in the question\n",
    "- We could ask for different types of questions - trivia, history, science\n",
    "- We could also vary the specific details that we ask for in the question\n",
    "\n",
    "let's see this in action below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Question: What is the capital and largest city of France?\n",
      " \n",
      "Generated Question: What are some of the most notable cultural landmarks in Paris that contribute to its reputation as a global center for art and culture?\n",
      " \n",
      "Generated Question: What factors contribute to Paris's status as a major global center for art, fashion, gastronomy, and culture, despite its population of 2.1 million residents?\n",
      " \n",
      "Generated Question: What makes Paris such a hotspot for art lovers?\n",
      " \n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import instructor\n",
    "from pydantic import BaseModel\n",
    "import random\n",
    "\n",
    "client = instructor.from_openai(OpenAI())\n",
    "\n",
    "\n",
    "class Question(BaseModel):\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Sample information about Paris\n",
    "paris_info = \"\"\"\n",
    "Paris is the capital and largest city of France, with an estimated population of 2.1 million residents.\n",
    "Located in northern France, it is a major global center for art, fashion, gastronomy and culture.\n",
    "The city is known for its iconic landmarks including:\n",
    "- The Eiffel Tower\n",
    "- The Louvre Museum\n",
    "- Notre-Dame Cathedral\n",
    "- Arc de Triomphe\n",
    "Paris is also home to world-class universities, financial institutions, and is one of the world's leading tourist destinations.\n",
    "\"\"\"\n",
    "# Define specific questions about landmarks and attractions\n",
    "questions = [\"historical\", \"cultural\", \"geographical\", \"art\"]\n",
    "tones = [\"curt\", \"formal\", \"technical\", \"casual\"]\n",
    "for question, tone in zip(questions, tones):\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"You're a helpful assistant that can generate detailed questions and answers based on the information provided below. Make sure that the question you're generated is a question pertaining to {question} and written in a {tone} tone.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": f\"Context: {paris_info}\"},\n",
    "        ],\n",
    "        response_model=Question,\n",
    "    )\n",
    "\n",
    "    print(f\"Generated Question: {resp.question}\")\n",
    "    # print(f\"Answer: {resp.answer}\")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By being more tactful about the variation that we introduce, we can generate a much more diverse set of questions. This is useful for us because it allows us to generate a much more representative set of questions that we can use to evaluate our retrieval.\n",
    "\n",
    "Now that we've seen what are some of the issues behind using synthetic questions and what they are, let's see how we can generate our first set of synthetic questions using the items in our dataset for reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Synthetic Questions\n",
    "\n",
    "Going back to the previous section, what we want is to generate a set of questions that are going to include a mix of the following conditions\n",
    "\n",
    "- Questions about price\n",
    "- Questions about material\n",
    "- Availability constraints\n",
    "\n",
    "Let's see how we can do so for a single item in our dataset by reading in an item from our local `lancedb` database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivanleo/Documents/coding/ai-engineer-summit/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from lancedb import connect\n",
    "\n",
    "db = connect(\"./lancedb\")\n",
    "table = db.open_table(\"items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1,\n",
       " 'title': 'Lace Detail Sleeveless Top',\n",
       " 'description': \"Elevate your casual wardrobe with this elegant sleeveless top featuring intricate lace detailing at the neckline. Perfect for both day and night, it's crafted from a soft, breathable fabric for all-day comfort.\",\n",
       " 'brand': 'H&M',\n",
       " 'category': 'Tops',\n",
       " 'product_type': 'Tank Tops',\n",
       " 'attributes': '[{\"name\": \"Sleeve Length\", \"value\": \"Sleeveless\"}, {\"name\": \"Neckline\", \"value\": \"Crew Neck\"}]',\n",
       " 'material': 'Cotton',\n",
       " 'pattern': 'Solid',\n",
       " 'price': 181.04,\n",
       " 'vector': array([ 0.09288761,  0.04094775, -0.00205971, ..., -0.00216066,\n",
       "         0.02410295, -0.02825646], shape=(1536,), dtype=float32),\n",
       " 'in_stock': False}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = table.to_pandas().head(4)\n",
    "item = df.iloc[0].to_dict()\n",
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import instructor\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class UserQuery(BaseModel):\n",
    "    chain_of_thought: str\n",
    "    query: str\n",
    "\n",
    "\n",
    "async def generate_synthetic_question(client: instructor.AsyncInstructor, item: dict):\n",
    "    condition = [\n",
    "        \"price\",\n",
    "        \"material\",\n",
    "        \"whether it's in stock\",\n",
    "    ]\n",
    "    return await client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"\n",
    "Generate a natural shopping query where this item would be the perfect recommendation. This query should be a query about the {{ query_type }} of this specific item.\n",
    "\n",
    "Item Details:\n",
    "- Title: {{title}}\n",
    "- Description: {{description}}\n",
    "- Brand: {{brand}}\n",
    "- Material: {{material}}\n",
    "- Pattern: {{pattern}}\n",
    "- Attributes: {{attributes}}\n",
    "- Price: {{price}}\n",
    "- Category: {{category}}\n",
    "- Product Type: {{product_type}}\n",
    "- In Stock : {{ stock_status }}\n",
    "\n",
    "Make sure that \n",
    "\n",
    "Requirements:\n",
    "- Query should be 20-30 words\n",
    "- Conversational tone\n",
    "- The query should describe the aspects above that make the item above a perfect match for the user's requirements\n",
    "- Try to mention things which might be synonyms for the item and avoid mentioning it directly. Instead we should use specific attributes that the item has in order to make it a good fit. Make sure to use the exact attribute name so that it's unambigious\n",
    "- for the price range, keep it to 15 bucks on either side of the price max\n",
    "- Do not mention the item's name or brand in the query itself\n",
    "\n",
    "Remember: The query should describe what someone would be looking for if this exact item would be their perfect match!\n",
    "\"\"\",\n",
    "            }\n",
    "        ],\n",
    "        context={\n",
    "            \"query_type\": random.choice(condition),\n",
    "            \"stock_status\": item[\"in_stock\"],\n",
    "            \"title\": item[\"title\"],\n",
    "            \"description\": item[\"description\"],\n",
    "            \"brand\": item[\"brand\"],\n",
    "            \"material\": item[\"material\"],\n",
    "            \"pattern\": item[\"pattern\"],\n",
    "            \"attributes\": item[\"attributes\"],\n",
    "            \"price\": item[\"price\"],\n",
    "            \"category\": item[\"category\"],\n",
    "            \"product_type\": item[\"product_type\"],\n",
    "        },\n",
    "        response_model=UserQuery,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">UserQuery</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">chain_of_thought</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The shopper is looking for a stylish and comfortable sleeveless top with lace detailing, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">suitable for day and night outfits, within a price range around $166 to $196. This specific item is not in stock, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and the user needs to know about its availability.'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">query</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Is there a solid pattern, sleeveless top with a lace neckline, comfortable for day and night wear, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">available for purchase around $181?'</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mUserQuery\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mchain_of_thought\u001b[0m=\u001b[32m'The shopper is looking for a stylish and comfortable sleeveless top with lace detailing, \u001b[0m\n",
       "\u001b[32msuitable for day and night outfits, within a price range around $166 to $196. This specific item is not in stock, \u001b[0m\n",
       "\u001b[32mand the user needs to know about its availability.'\u001b[0m,\n",
       "    \u001b[33mquery\u001b[0m=\u001b[32m'Is there a solid pattern, sleeveless top with a lace neckline, comfortable for day and night wear, \u001b[0m\n",
       "\u001b[32mavailable for purchase around $181?'\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import instructor\n",
    "from openai import AsyncOpenAI\n",
    "from rich import print\n",
    "\n",
    "client = instructor.from_openai(AsyncOpenAI())\n",
    "\n",
    "print(await generate_synthetic_question(client, item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we’ve explored how to generate synthetic queries—and importantly, how to introduce the controlled variations needed to mimic real user inputs—let’s shift our focus to measurement. \n",
    "\n",
    "In the next section, we’ll introduce key retrieval metrics like Recall and Mean Reciprocal Rank (MRR) to objectively assess the performance of our retrieval system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Metrics\n",
    "\n",
    "Let's start by looking at a few metrics that we can use to evaluate the quality of our retrieval.\n",
    "\n",
    "### Key Retrieval Metrics\n",
    "\n",
    "**Precision** measures how many of our retrieved items are actually relevant:\n",
    "\n",
    "$$ \\text{Precision} = \\frac{\\text{Number of Relevant Items Retrieved}}{\\text{Total Number of Retrieved Items}} $$ \n",
    "\n",
    "For example, if your system retrieves 10 documents but only 5 are relevant, that's 50% precision. Low precision indicates your system is wasting resources processing irrelevant content.\n",
    "\n",
    "**Recall** measures how many of the total relevant items we managed to find:\n",
    "\n",
    "$$ \\text{Recall} = \\frac{\\text{Number of Relevant Items Retrieved}}{\\text{Total Number of Relevant Items}} $$ \n",
    "\n",
    "If there are 20 relevant documents in your database but you only retrieve 10 of them, that's 50% recall. Low recall suggests you're missing important information.\n",
    "\n",
    "**Mean Reciprocal Rank (MRR)** measures the average reciprocal rank of the first relevant document:\n",
    "\n",
    "$$ \\text{MRR} = \\frac{\\sum\\_{i=1}^{n} \\frac{1}{rank(i)}}{n} $$\n",
    "\n",
    "The higher the MRR, the better. It penalizes systems that retrieve irrelevant documents early in the list.\n",
    "\n",
    "In practice, we often measure these metrics at specific cutoff points (like top-5 or top-10 results), denoted as Precision@K or Recall@K. These cut-off points are always going to be determined by some specific business or product requirement. Here are some practical examples of how we might use these metrics:\n",
    "\n",
    "\n",
    "\n",
    "| **Use Case**           | **Primary Metrics**  | **Reasoning**                                                                                                          |\n",
    "|------------------------|----------------------|------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Retrieval**          | Recall, MRR          | Ensures the correct context is provided by retrieving all relevant items and ranking them highly.                      |\n",
    "| **Tool Calling**       | Precision, Recall    | Guarantees that all necessary tools are invoked while avoiding irrelevant tool calls that waste resources.             |\n",
    "| **Metadata Filtering** | Recall, MRR          | Focuses on improving the quality of returned results by effectively filtering a large dataset for the most pertinent items. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mrr(predictions: list[str], gt: list[str]):\n",
    "    mrr = 0\n",
    "    for label in gt:\n",
    "        if label in predictions:\n",
    "            # Find the relevant item that has the smallest index\n",
    "            mrr = max(mrr, 1 / (predictions.index(label) + 1))\n",
    "    return mrr\n",
    "\n",
    "\n",
    "def calculate_recall(predictions: list[str], gt: list[str]):\n",
    "    # Calculate the proportion of relevant items that were retrieved\n",
    "    return len([label for label in gt if label in predictions]) / len(gt)\n",
    "\n",
    "\n",
    "def calculate_precision(predictions: list[str], gt: list[str]):\n",
    "    # Calculate the proportion of retrieved items that are relevant\n",
    "    return len([label for label in predictions if label in gt]) / len(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd ideally also like to compute the precision and recall at a variety of different cutoff points. Let's see how we can do this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools \n",
    "\n",
    "def get_metrics_at_k(\n",
    "    metrics: list[str],\n",
    "    sizes: list[int],\n",
    "):\n",
    "    metric_to_score_fn = {\n",
    "        \"mrr\": calculate_mrr,\n",
    "        \"recall\": calculate_recall,\n",
    "    }\n",
    "\n",
    "    for metric in metrics:\n",
    "        if metric not in metric_to_score_fn:\n",
    "            raise ValueError(f\"Metric {metric} not supported\")\n",
    "\n",
    "    eval_metrics = [(metric, metric_to_score_fn[metric]) for metric in metrics]\n",
    "\n",
    "    return {\n",
    "        f\"{metric_name}@{size}\": lambda predictions, gt, m=metric_fn, s=size: (\n",
    "            lambda p, g: m(p[:s], g)\n",
    "        )(predictions, gt)\n",
    "        for (metric_name, metric_fn), size in itertools.product(eval_metrics, sizes)\n",
    "    }\n",
    "\n",
    "metrics = get_metrics_at_k(metrics=[\"mrr\", \"recall\"], sizes=[5, 10, 25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then compute apply this metrics list on a set of retrieved items as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mrr@5': 0,\n",
       " 'mrr@10': 0.1,\n",
       " 'mrr@25': 0.1,\n",
       " 'recall@5': 0.0,\n",
       " 'recall@10': 1.0,\n",
       " 'recall@25': 1.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "retrieved_item_ids = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
    "desired_item = [10]\n",
    "\n",
    "scores = {metric: score_fn(retrieved_item_ids, desired_item) for metric, score_fn in metrics.items()}\n",
    "scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we've computed the metrics for a variety of different cutoff points. This is a function that we can parameterise and apply to calculate mrr and recall at a variety of different cutoff points.\n",
    "\n",
    "let's now see how we can apply this to a single query below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's a good fit for our use case?\n",
    "\n",
    "In our RAG application, we're going to be using `recall` and `MRR` as our primary metrics. This is for two reasons\n",
    "\n",
    "1. **Recall** : We want to make sure that for the synthetic question we generate for that specific item, we're able to retrieve it. Recall will determine whether this is the case for us\n",
    "\n",
    "2. **MRR** : More often than not, we're also going to be displaying these suggested items to the user. In this case, we'd want to make sure that the relevant item is ranked as highly as possible so that the user has a high chance of clicking on it. MRR will help us understand how well we're able to rank the relevant item\n",
    "\n",
    "With a clear understanding of metrics such as Recall and MRR, we’re ready to put these numbers into practice. In the next section, we'll evaluate our existing retrieval system. We'll first demonstrate how to calculate these metrics for a single query before we scale it out to our entire dataset. \n",
    "\n",
    "This will allow us to establish a robust baseline which will serve as our reference point for measuring future improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing a Baseline\n",
    "\n",
    "### Evaluating A Single Query\n",
    "\n",
    "We've generated a small dataset of synthetic questions ahead of time so that you can measure the performance of your retrieval system. Let's read in the dataset from the `queries.json` file and then use them to compute our baseline metrics.\n",
    "\n",
    "Let's see how we might evaluate the recall and MRR for a single query. We'll use the query at index 3 in our dataset which our retrieval system will struggle with slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load in queries that we generated previously\n",
    "with open(\"./data/queries.json\", \"r\") as f:\n",
    "    queries = [json.loads(line) for line in f]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Searching for a cropped cotton top with short sleeves and a stylish turtleneck for casual outings and everyday wear, ideally priced below $400.',\n",
       " 'title': \"Fila Women's Cropped Logo T-Shirt\",\n",
       " 'brand': 'Fila',\n",
       " 'description': \"Elevate your casual look with the Fila Women's Cropped Logo T-Shirt. Featuring the iconic Fila logo in a sleek design, this short-sleeve turtleneck adds a touch of sporty elegance to any ensemble.\",\n",
       " 'category': 'Tops',\n",
       " 'product_type': 'T-Shirts',\n",
       " 'attributes': '[{\"name\": \"Sleeve Length\", \"value\": \"Short Sleeve\"}, {\"name\": \"Neckline\", \"value\": \"Turtleneck\"}, {\"name\": \"Fit\", \"value\": \"Cropped\"}]',\n",
       " 'material': 'Cotton',\n",
       " 'pattern': 'Solid',\n",
       " 'id': 4,\n",
       " 'price': 374.89}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see how well our retrieval system performs of this specific query. We'll do so by fetching the top 25 items from our retrieval system and then computing the recall and MRR at a variety of cutoff points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_query = queries[3]\n",
    "retrieved_item_ids = [item['id'] for item in table.search(target_query[\"query\"]).limit(25).to_list()]\n",
    "desired_item = [target_query[\"id\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've fetched the top 25 items from our retrieval system, we can then compute the recall and MRR at a variety of cutoff points as seen below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mrr@5': 0,\n",
       " 'mrr@10': 0,\n",
       " 'mrr@15': 0,\n",
       " 'mrr@20': 0,\n",
       " 'mrr@25': 0,\n",
       " 'recall@5': 0.0,\n",
       " 'recall@10': 0.0,\n",
       " 'recall@15': 0.0,\n",
       " 'recall@20': 0.0,\n",
       " 'recall@25': 0.0}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = get_metrics_at_k(metrics=[\"mrr\", \"recall\"], sizes=[5, 10, 15, 20, 25])\n",
    "scores = {metric: score_fn(retrieved_item_ids, desired_item) for metric, score_fn in metrics.items()}\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our retrieval system wasn't able to retrieve the relevant items for this specific query itself. Let's take a closer look to see what was retrieved and what the query was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Searching for a cropped cotton top with short sleeves and a stylish turtleneck for casual outings and everyday \n",
       "wear, ideally priced below $<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">400</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Searching for a cropped cotton top with short sleeves and a stylish turtleneck for casual outings and everyday \n",
       "wear, ideally priced below $\u001b[1;36m400\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich import print\n",
    "\n",
    "print(target_query['query'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "      <th>product_type</th>\n",
       "      <th>price</th>\n",
       "      <th>material</th>\n",
       "      <th>attributes</th>\n",
       "      <th>_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>145</td>\n",
       "      <td>Off-Shoulder Crop Top</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "      <td>184.61</td>\n",
       "      <td>Cotton</td>\n",
       "      <td>[{\"name\": \"Sleeve Length\", \"value\": \"Short Sle...</td>\n",
       "      <td>0.832879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>130</td>\n",
       "      <td>Women's Cutout Cropped Top</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "      <td>222.68</td>\n",
       "      <td>Cotton</td>\n",
       "      <td>[{\"name\": \"Sleeve Length\", \"value\": \"Long Slee...</td>\n",
       "      <td>0.886718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>73</td>\n",
       "      <td>Smocked Button Front Crop Top</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Tank Tops</td>\n",
       "      <td>355.15</td>\n",
       "      <td>Cotton</td>\n",
       "      <td>[{\"name\": \"Sleeve Length\", \"value\": \"Sleeveles...</td>\n",
       "      <td>0.895110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Plaid Crop Top</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Tank Tops</td>\n",
       "      <td>261.05</td>\n",
       "      <td>Cotton</td>\n",
       "      <td>[{\"name\": \"Sleeve Length\", \"value\": \"Sleeveles...</td>\n",
       "      <td>0.915811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>108</td>\n",
       "      <td>Lace-Up Cropped Blouse</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "      <td>158.65</td>\n",
       "      <td>Cotton</td>\n",
       "      <td>[{\"name\": \"Sleeve Length\", \"value\": \"Short Sle...</td>\n",
       "      <td>0.931424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>Fila Women's Cropped Logo T-Shirt</td>\n",
       "      <td>Tops</td>\n",
       "      <td>T-Shirts</td>\n",
       "      <td>374.89</td>\n",
       "      <td>Cotton</td>\n",
       "      <td>[{\"name\": \"Sleeve Length\", \"value\": \"Short Sle...</td>\n",
       "      <td>0.949217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>85</td>\n",
       "      <td>Women's Sleeveless Tie-Waist Crop Top</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Tank Tops</td>\n",
       "      <td>163.79</td>\n",
       "      <td>Spandex</td>\n",
       "      <td>[{\"name\": \"Sleeve Length\", \"value\": \"Sleeveles...</td>\n",
       "      <td>0.950213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>172</td>\n",
       "      <td>Patterned Long Sleeve Top</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Sweaters</td>\n",
       "      <td>382.99</td>\n",
       "      <td>Cotton</td>\n",
       "      <td>[{\"name\": \"Sleeve Length\", \"value\": \"Long Slee...</td>\n",
       "      <td>0.960017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>44</td>\n",
       "      <td>Women's Black Wrap Crop Top</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "      <td>99.28</td>\n",
       "      <td>Cotton</td>\n",
       "      <td>[{\"name\": \"Sleeve Length\", \"value\": \"Long Slee...</td>\n",
       "      <td>0.963834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>51</td>\n",
       "      <td>Women's Cropped Logo T-Shirt</td>\n",
       "      <td>Tops</td>\n",
       "      <td>T-Shirts</td>\n",
       "      <td>226.56</td>\n",
       "      <td>Cotton</td>\n",
       "      <td>[{\"name\": \"Sleeve Length\", \"value\": \"Short Sle...</td>\n",
       "      <td>0.969768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>67</td>\n",
       "      <td>Women's Long Sleeve Turtleneck Top</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Sweaters</td>\n",
       "      <td>154.75</td>\n",
       "      <td>Cotton</td>\n",
       "      <td>[{\"name\": \"Sleeve Length\", \"value\": \"Long Slee...</td>\n",
       "      <td>0.974626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>94</td>\n",
       "      <td>Women's Cropped Tank Top</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Tank Tops</td>\n",
       "      <td>163.14</td>\n",
       "      <td>Polyester</td>\n",
       "      <td>[{\"name\": \"Sleeve Length\", \"value\": \"Sleeveles...</td>\n",
       "      <td>0.976427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>182</td>\n",
       "      <td>Ribbed Short Sleeve Top</td>\n",
       "      <td>Tops</td>\n",
       "      <td>T-Shirts</td>\n",
       "      <td>56.94</td>\n",
       "      <td>Cotton</td>\n",
       "      <td>[{\"name\": \"Sleeve Length\", \"value\": \"Short Sle...</td>\n",
       "      <td>0.977656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>82</td>\n",
       "      <td>Calvin Klein Women's Contrast Collar T-Shirt</td>\n",
       "      <td>Tops</td>\n",
       "      <td>T-Shirts</td>\n",
       "      <td>272.54</td>\n",
       "      <td>Cotton</td>\n",
       "      <td>[{\"name\": \"Sleeve Length\", \"value\": \"Short Sle...</td>\n",
       "      <td>0.978985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>138</td>\n",
       "      <td>Sleeveless Black Top</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Tank Tops</td>\n",
       "      <td>298.01</td>\n",
       "      <td>Polyester</td>\n",
       "      <td>[{\"name\": \"Sleeve Length\", \"value\": \"Sleeveles...</td>\n",
       "      <td>0.980849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>159</td>\n",
       "      <td>Scallop Trim Short Sleeve Knit Top</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "      <td>207.28</td>\n",
       "      <td>Cotton</td>\n",
       "      <td>[{\"name\": \"Sleeve Length\", \"value\": \"Short Sle...</td>\n",
       "      <td>0.982479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>26</td>\n",
       "      <td>Striped Long Sleeve Top</td>\n",
       "      <td>Tops</td>\n",
       "      <td>T-Shirts</td>\n",
       "      <td>81.09</td>\n",
       "      <td>Cotton</td>\n",
       "      <td>[{\"name\": \"Sleeve Length\", \"value\": \"Long Slee...</td>\n",
       "      <td>0.985236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>32</td>\n",
       "      <td>Striped Long Sleeve Top</td>\n",
       "      <td>Tops</td>\n",
       "      <td>T-Shirts</td>\n",
       "      <td>221.78</td>\n",
       "      <td>Cotton</td>\n",
       "      <td>[{\"name\": \"Sleeve Length\", \"value\": \"Long Slee...</td>\n",
       "      <td>0.995736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>93</td>\n",
       "      <td>Long Sleeve Navy Top</td>\n",
       "      <td>Tops</td>\n",
       "      <td>T-Shirts</td>\n",
       "      <td>80.13</td>\n",
       "      <td>Cotton</td>\n",
       "      <td>[{\"name\": \"Sleeve Length\", \"value\": \"Long Slee...</td>\n",
       "      <td>0.995753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>98</td>\n",
       "      <td>Embellished Navy Cap Sleeve Top</td>\n",
       "      <td>Tops</td>\n",
       "      <td>T-Shirts</td>\n",
       "      <td>306.57</td>\n",
       "      <td>Cotton</td>\n",
       "      <td>[{\"name\": \"Sleeve Length\", \"value\": \"Short Sle...</td>\n",
       "      <td>1.000904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                         title category product_type  \\\n",
       "0   145                         Off-Shoulder Crop Top     Tops      Blouses   \n",
       "1   130                    Women's Cutout Cropped Top     Tops      Blouses   \n",
       "2    73                 Smocked Button Front Crop Top     Tops    Tank Tops   \n",
       "3     5                                Plaid Crop Top     Tops    Tank Tops   \n",
       "4   108                        Lace-Up Cropped Blouse     Tops      Blouses   \n",
       "5     4             Fila Women's Cropped Logo T-Shirt     Tops     T-Shirts   \n",
       "6    85         Women's Sleeveless Tie-Waist Crop Top     Tops    Tank Tops   \n",
       "7   172                     Patterned Long Sleeve Top     Tops     Sweaters   \n",
       "8    44                   Women's Black Wrap Crop Top     Tops      Blouses   \n",
       "9    51                  Women's Cropped Logo T-Shirt     Tops     T-Shirts   \n",
       "10   67            Women's Long Sleeve Turtleneck Top     Tops     Sweaters   \n",
       "11   94                      Women's Cropped Tank Top     Tops    Tank Tops   \n",
       "12  182                       Ribbed Short Sleeve Top     Tops     T-Shirts   \n",
       "13   82  Calvin Klein Women's Contrast Collar T-Shirt     Tops     T-Shirts   \n",
       "14  138                          Sleeveless Black Top     Tops    Tank Tops   \n",
       "15  159            Scallop Trim Short Sleeve Knit Top     Tops      Blouses   \n",
       "16   26                       Striped Long Sleeve Top     Tops     T-Shirts   \n",
       "17   32                       Striped Long Sleeve Top     Tops     T-Shirts   \n",
       "18   93                          Long Sleeve Navy Top     Tops     T-Shirts   \n",
       "19   98               Embellished Navy Cap Sleeve Top     Tops     T-Shirts   \n",
       "\n",
       "     price   material                                         attributes  \\\n",
       "0   184.61     Cotton  [{\"name\": \"Sleeve Length\", \"value\": \"Short Sle...   \n",
       "1   222.68     Cotton  [{\"name\": \"Sleeve Length\", \"value\": \"Long Slee...   \n",
       "2   355.15     Cotton  [{\"name\": \"Sleeve Length\", \"value\": \"Sleeveles...   \n",
       "3   261.05     Cotton  [{\"name\": \"Sleeve Length\", \"value\": \"Sleeveles...   \n",
       "4   158.65     Cotton  [{\"name\": \"Sleeve Length\", \"value\": \"Short Sle...   \n",
       "5   374.89     Cotton  [{\"name\": \"Sleeve Length\", \"value\": \"Short Sle...   \n",
       "6   163.79    Spandex  [{\"name\": \"Sleeve Length\", \"value\": \"Sleeveles...   \n",
       "7   382.99     Cotton  [{\"name\": \"Sleeve Length\", \"value\": \"Long Slee...   \n",
       "8    99.28     Cotton  [{\"name\": \"Sleeve Length\", \"value\": \"Long Slee...   \n",
       "9   226.56     Cotton  [{\"name\": \"Sleeve Length\", \"value\": \"Short Sle...   \n",
       "10  154.75     Cotton  [{\"name\": \"Sleeve Length\", \"value\": \"Long Slee...   \n",
       "11  163.14  Polyester  [{\"name\": \"Sleeve Length\", \"value\": \"Sleeveles...   \n",
       "12   56.94     Cotton  [{\"name\": \"Sleeve Length\", \"value\": \"Short Sle...   \n",
       "13  272.54     Cotton  [{\"name\": \"Sleeve Length\", \"value\": \"Short Sle...   \n",
       "14  298.01  Polyester  [{\"name\": \"Sleeve Length\", \"value\": \"Sleeveles...   \n",
       "15  207.28     Cotton  [{\"name\": \"Sleeve Length\", \"value\": \"Short Sle...   \n",
       "16   81.09     Cotton  [{\"name\": \"Sleeve Length\", \"value\": \"Long Slee...   \n",
       "17  221.78     Cotton  [{\"name\": \"Sleeve Length\", \"value\": \"Long Slee...   \n",
       "18   80.13     Cotton  [{\"name\": \"Sleeve Length\", \"value\": \"Long Slee...   \n",
       "19  306.57     Cotton  [{\"name\": \"Sleeve Length\", \"value\": \"Short Sle...   \n",
       "\n",
       "    _distance  \n",
       "0    0.832879  \n",
       "1    0.886718  \n",
       "2    0.895110  \n",
       "3    0.915811  \n",
       "4    0.931424  \n",
       "5    0.949217  \n",
       "6    0.950213  \n",
       "7    0.960017  \n",
       "8    0.963834  \n",
       "9    0.969768  \n",
       "10   0.974626  \n",
       "11   0.976427  \n",
       "12   0.977656  \n",
       "13   0.978985  \n",
       "14   0.980849  \n",
       "15   0.982479  \n",
       "16   0.985236  \n",
       "17   0.995736  \n",
       "18   0.995753  \n",
       "19   1.000904  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.search(target_query[\"query\"]).select([\"id\", \"title\",\"category\", \"product_type\",\"price\",\"material\",\"attributes\"]).limit(25).to_pandas().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that while the price was adhered to, we've completely missed the mark on the other aspects. \n",
    "\n",
    "1. We've got a mix of different materials - even though most of cotton, we do have some Polyester and Spandex in the mix\n",
    "2. We've got a mix of different product types - we've got a t-shirt, a dress and a skirt when what we wanted was a T-Shirt\n",
    "3. We've also got a mix of different attributes - we wanted short sleeved shirts but we've got some long sleeved shirts in the mix as well\n",
    "\n",
    "As a result, our original item didn't appear in the top 25 results. Let's now compute the recall and MRR for all of our given queries and see how we perform as a whole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Our Baseline\n",
    "\n",
    "We'll use pandas to store the results of our computations and then dump the results into a simple .csv file for future reference. To run our code in Parallel, we'll use a ThreadPoolExecutor to run these async calls in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mrr@5': 0,\n",
       " 'mrr@10': 0,\n",
       " 'mrr@15': 0,\n",
       " 'mrr@20': 0,\n",
       " 'mrr@25': 0,\n",
       " 'recall@5': 0.0,\n",
       " 'recall@10': 0.0,\n",
       " 'recall@15': 0.0,\n",
       " 'recall@20': 0.0,\n",
       " 'recall@25': 0.0}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lancedb.table import Table\n",
    "\n",
    "def retrieve_items(table:Table,item:dict, metrics:dict):\n",
    "    retrieved_item_ids = [item['id'] for item in table.search(item['query']).limit(25).to_list()]\n",
    "    desired_item = [item['id']]\n",
    "    return {metric: score_fn(retrieved_item_ids, desired_item) for metric, score_fn in metrics.items()}\n",
    "\n",
    "\n",
    "db = connect(\"./lancedb\")\n",
    "table = db.open_table(\"items\")\n",
    "retrieve_items(table,queries[3],metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mrr@5        0.47\n",
       "mrr@10       0.49\n",
       "mrr@15       0.49\n",
       "mrr@20       0.49\n",
       "mrr@25       0.49\n",
       "recall@5     0.63\n",
       "recall@10    0.76\n",
       "recall@15    0.79\n",
       "recall@20    0.84\n",
       "recall@25    0.84\n",
       "dtype: float64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pandas as pd\n",
    "\n",
    "# Create a ThreadPoolExecutor to run queries in parallel\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    # Map the retrieve_items function across all queries\n",
    "    results = list(executor.map(\n",
    "        lambda q: retrieve_items(table, q, metrics),\n",
    "        queries\n",
    "    ))\n",
    "\n",
    "# Convert results to DataFrame for analysis\n",
    "results_df = pd.DataFrame(results)\n",
    "round(results_df.mean(),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we've now computed an initial set of baseline scores for our retrieval system using metrics like recall and MRR. In the next section, we'll see how we can improve the performance of our retrieval system by enriching item descriptions with metdata to boost retrieval evals. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Better Captioning\n",
    "\n",
    "One of the easiest changes that we can make to our retrieval system is to modify the text that we're using for similarity search. In this section, we'll see how we can improve performance by simplfy concatenating metadata information to the item description and using this new description for similarity search.\n",
    "\n",
    "We'll start by loading in our dataset using `lancedb` and then we'll create a new table that embeds the item description and metadata information together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">191</span> rows in the table\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m191\u001b[0m rows in the table\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import required libraries for LanceDB and data handling\n",
    "from lancedb.pydantic import LanceModel, Vector\n",
    "import lancedb\n",
    "from lancedb.embeddings import get_registry\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the e-commerce product dataset\n",
    "dataset = load_dataset(\"ivanleomk/ai-engineer-summit-ecommerce-taxonomy\")[\"train\"]\n",
    "\n",
    "# Initialize the OpenAI text embedding model\n",
    "func = get_registry().get(\"openai\").create(name=\"text-embedding-3-small\")\n",
    "\n",
    "\n",
    "# Define a Pydantic model for our database schema\n",
    "class Item(LanceModel):\n",
    "    id: int\n",
    "    title: str\n",
    "    description: str = func.SourceField()  # Field that will be embedded\n",
    "    brand: str\n",
    "    category: str\n",
    "    product_type: str\n",
    "    attributes: str\n",
    "    material: str\n",
    "    pattern: str\n",
    "    price: float\n",
    "    vector: Vector(func.ndims()) = func.VectorField()  # Store the embedding vector\n",
    "    in_stock: bool\n",
    "\n",
    "\n",
    "# Connect to LanceDB\n",
    "db = lancedb.connect(\"./lancedb\")\n",
    "table_name = \"items_better_captioning\"\n",
    "\n",
    "\n",
    "# Create and populate table if it doesn't exist\n",
    "if table_name not in db.table_names():\n",
    "    # Create new table with our schema\n",
    "    concat_table = db.create_table(table_name, schema=Item, mode=\"overwrite\")\n",
    "    entries = []\n",
    "    \n",
    "    # Process each row in the dataset\n",
    "    for row in dataset:\n",
    "        # Create enhanced description by combining multiple fields\n",
    "        entries.append(\n",
    "            {\n",
    "                \"id\": row[\"id\"],\n",
    "                \"title\": row[\"title\"],\n",
    "                \"description\": f\"\"\"\n",
    "title: {row[\"title\"]}\n",
    "description: {row[\"description\"]}\n",
    "brand: {row[\"brand\"]}\n",
    "category: {row[\"category\"]}\n",
    "product_type: {row[\"product_type\"]}\n",
    "price: {row[\"price\"]}\n",
    "attributes: {json.dumps(row[\"attributes\"])}\n",
    "\"\"\".strip(),  # Combine metadata with description for better search\n",
    "                \"brand\": row[\"brand\"],\n",
    "                \"category\": row[\"category\"],\n",
    "                \"product_type\": row[\"product_type\"],\n",
    "                \"attributes\": row[\"attributes\"],\n",
    "                \"material\": row[\"material\"],\n",
    "                \"pattern\": row[\"pattern\"],\n",
    "                \"price\": row[\"price\"],\n",
    "                \"in_stock\": row[\"in_stock\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Add all entries to the table\n",
    "    concat_table.add(entries)\n",
    "\n",
    "# Open the table for querying\n",
    "concat_table = db.open_table(table_name)\n",
    "print(f\"{table.count_rows()} rows in the table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created our new table, let's see how much of an improvement we get by using this new description using vector search. We'll use the same queries that we used previously and compute the recall and MRR at a variety of cutoff points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mrr@5        0.56\n",
       "mrr@10       0.58\n",
       "mrr@15       0.58\n",
       "mrr@20       0.58\n",
       "mrr@25       0.58\n",
       "recall@5     0.71\n",
       "recall@10    0.84\n",
       "recall@15    0.87\n",
       "recall@20    0.89\n",
       "recall@25    0.92\n",
       "dtype: float64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pandas as pd\n",
    "\n",
    "# Create a ThreadPoolExecutor to run queries in parallel\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    # Map the retrieve_items function across all queries\n",
    "    results = list(executor.map(\n",
    "        lambda q: retrieve_items(concat_table, q, metrics),\n",
    "        queries\n",
    "    ))\n",
    "\n",
    "# Convert results to DataFrame for analysis\n",
    "results_df = pd.DataFrame(results)\n",
    "round(results_df.mean(),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we see an average increase of ~18% for MRR and ~10% for recall across all values of `k` for this new improved description that we're embedding.\n",
    "\n",
    "| Metric | Semantic Search (Baseline) | Semantic Search + Metadata |\n",
    "|---------|--------------------------|----------------------------------|\n",
    "| MRR@5   | 0.47 | 0.56 (+19.15%) |\n",
    "| MRR@10  | 0.49 | 0.58 (+18.43%) |\n",
    "| MRR@15  | 0.49 | 0.58 (+18.42%) |\n",
    "| MRR@20  | 0.49 | 0.58 (+18.21%) |\n",
    "| MRR@25  | 0.49 | 0.58 (+18.21%) |\n",
    "| Recall@5  | 0.63 | 0.71 (+12.50%) |\n",
    "| Recall@10 | 0.76 | 0.84 (+10.34%) |\n",
    "| Recall@15 | 0.79 | 0.87 (+10.00%) |\n",
    "| Recall@20 | 0.84 | 0.92 (+9.52%) |\n",
    "| Recall@25 | 0.84 | 0.92 (+9.52%) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of simply hoping that our tweaks will work or relying on arbitrary advice, we now have measurable evidence of improvement. For example, by concatenating metadata to the item description, we’ve boosted our MRR by 19% and our recall by 9% on average. This marks a significant shift—from a mindset of “I hope this works” to one where we can quantify the benefits and make informed, objective decisions.\n",
    "\n",
    "This allows us to prioritise changes that make a difference and give us the biggest leverage rather than hoping it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we explored a systematic approach to evaluating the retrieval component of your RAG application. By bootstrapping our evaluation datasets with synthetic queries and leveraging key metrics like Recall and Mean Reciprocal Rank (MRR), we established a robust performance baseline. We discussed the challenges of generating diverse queries and demonstrated how controlled variation in query generation can lead to a more representative evaluation of our system.\n",
    "\n",
    "Our approach allowed us to quantify the impact of improvements—such as enhancing item descriptions with richer metadata—which directly translated into measurable gains in recall and MRR. This systematic methodology not only helps identify high-leverage enhancements, like metadata filtering, but also extends to other areas such as fine-tuning embedding models or implementing re-ranking strategies. By quantifying the impact of each individual change, you can focus your efforts on modifications that truly move the needle.\n",
    "\n",
    "In the next notebook, we'll dive into metadata filtering. We'll also provide a sneak peek into how query understanding using function calling can further improve the performance of our retrieval system. This continuous, data-driven refinement ensures that every change you make is aligned with delivering significant, measurable improvements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
