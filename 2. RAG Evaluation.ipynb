{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2 : Evaluating Your RAG Application\n",
    "\n",
    "In the previous notebook, we looked at some of the key limitations of semantic search in finding the right information given some form of constraint. \n",
    "\n",
    "In this notebook, we'll examine how we can bootstrap simple evaluation datasets using synthetic data, introduce two metrics we can track in order to measure the quality of our retrieval and compute an initial baseline for our semantic search approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Datasets\n",
    "\n",
    "By leveraging language models, we can bootstrap evaluation datasets with synthetic data. This allows us to understand specific areas that our retrieval might suffer before we ship to production.\n",
    "\n",
    "Even after we ship to production, we can continously iterate on the synthetic datasets that we have by using user conversations as a reference for our language to generate more variations of. \n",
    "\n",
    "This allows us to iteratively update our synthetic datasets over time and make sure that they're representative of the queries we can expect in production. In this section, we'll examine this in 3 parts\n",
    "\n",
    "1. First we'll look briefly at what Synthetic Data is\n",
    "2. Then we'll talk about how we can generate an initial set of synthetic questions using the items in our dataset for reference\n",
    "3. Lastly, we'll show how we might use user queries as reference to generate more variations of\n",
    "\n",
    "At the end of this section you should have a clear idea of what synthetic data is and how you can generate your own variations of it.\n",
    "\n",
    "### What is Synthetic Data?\n",
    "\n",
    "Synthetic data is data that's not generated by a human. In our specific context here, we're refering to data that's generated by a language model itself.\n",
    "\n",
    "By leveraging language models to generate questions for us and thinking carefully about the constraints we want to apply to these questions, we can generate datasets that are much richer and diverse than what we could do ourselves. This is because of the sheer amount of data that's been used to train these language models in the first place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Question: What is the estimated population of Paris?\n",
      "Answer: The estimated population of Paris is 2.1 million residents.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import instructor\n",
    "from pydantic import BaseModel\n",
    "\n",
    "client = instructor.from_openai(OpenAI())\n",
    "\n",
    "\n",
    "class Question(BaseModel):\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Sample information about Paris\n",
    "paris_info = \"\"\"\n",
    "Paris is the capital and largest city of France, with an estimated population of 2.1 million residents.\n",
    "Located in northern France, it is a major global center for art, fashion, gastronomy and culture.\n",
    "The city is known for its iconic landmarks including:\n",
    "- The Eiffel Tower\n",
    "- The Louvre Museum\n",
    "- Notre-Dame Cathedral\n",
    "- Arc de Triomphe\n",
    "Paris is also home to world-class universities, financial institutions, and is one of the world's leading tourist destinations.\n",
    "\"\"\"\n",
    "\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant that can generate detailed questions and answers based on the information provided below. Include specific facts and figures where relevant.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": paris_info},\n",
    "    ],\n",
    "    response_model=Question,\n",
    ")\n",
    "\n",
    "print(f\"Generated Question: {resp.question}\")\n",
    "print(f\"Answer: {resp.answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we've generated our first synthetic question! We can see how by scaling this process out, we can generate a large amount of questions easily and efficiently. \n",
    "\n",
    "However, this doesn't come without some challenges - the biggest of which is that of diversity which we'll talk about in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diversity\n",
    "\n",
    "As mentioned above, the biggest challenge when generating synthetic data is that of diversity. If we pass the language model the same prompt over and over again, we'll get the same output every time. \n",
    "\n",
    "Let's see this in action below when we use the prompt above to generate 4 questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Question: What is the estimated population of Paris?\n",
      "Answer: The estimated population of Paris is 2.1 million residents.\n",
      "Generated Question: What is the estimated population of Paris?\n",
      "Answer: The estimated population of Paris is 2.1 million residents.\n",
      "Generated Question: What is the capital and largest city of France, and what is its estimated population?\n",
      "Answer: Paris is the capital and largest city of France, with an estimated population of 2.1 million residents.\n",
      "Generated Question: What is the estimated population of Paris?\n",
      "Answer: The estimated population of Paris is 2.1 million residents.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import instructor\n",
    "from pydantic import BaseModel\n",
    "\n",
    "client = instructor.from_openai(OpenAI())\n",
    "\n",
    "\n",
    "class Question(BaseModel):\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Sample information about Paris\n",
    "paris_info = \"\"\"\n",
    "Paris is the capital and largest city of France, with an estimated population of 2.1 million residents.\n",
    "Located in northern France, it is a major global center for art, fashion, gastronomy and culture.\n",
    "The city is known for its iconic landmarks including:\n",
    "- The Eiffel Tower\n",
    "- The Louvre Museum\n",
    "- Notre-Dame Cathedral\n",
    "- Arc de Triomphe\n",
    "Paris is also home to world-class universities, financial institutions, and is one of the world's leading tourist destinations.\n",
    "\"\"\"\n",
    "\n",
    "for _ in range(4):\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant that can generate detailed questions and answers based on the information provided below. Include specific facts and figures where relevant.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": paris_info},\n",
    "        ],\n",
    "        response_model=Question,\n",
    "    )\n",
    "\n",
    "    print(f\"Generated Question: {resp.question}\")\n",
    "    print(f\"Answer: {resp.answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that given the same prompt, the language model will generate very similar outputs over and over again. In order to combat this, we need to introduce some controlled variation into the process.\n",
    "\n",
    "Now this needs to be done in a way that makes sense. Just using random bits of information in the prompt won't introduce the variation you need to get a diverse set of questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Question: What is the capital and largest city of France, and what is its estimated population?\n",
      "Answer: Paris is the capital and largest city of France, with an estimated population of 2.1 million residents.\n",
      "Generated Question: What is the estimated population of Paris, and what are some of its iconic landmarks?\n",
      "Answer: Paris has an estimated population of 2.1 million residents. Some of its iconic landmarks include the Eiffel Tower, the Louvre Museum, Notre-Dame Cathedral, and the Arc de Triomphe.\n",
      "Generated Question: What is the estimated population of Paris?\n",
      "Answer: The estimated population of Paris is 2.1 million residents.\n",
      "Generated Question: What is the estimated population of Paris?\n",
      "Answer: The estimated population of Paris is 2.1 million residents.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import instructor\n",
    "from pydantic import BaseModel\n",
    "import random\n",
    "\n",
    "client = instructor.from_openai(OpenAI())\n",
    "\n",
    "\n",
    "class Question(BaseModel):\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Sample information about Paris\n",
    "paris_info = \"\"\"\n",
    "Paris is the capital and largest city of France, with an estimated population of 2.1 million residents.\n",
    "Located in northern France, it is a major global center for art, fashion, gastronomy and culture.\n",
    "The city is known for its iconic landmarks including:\n",
    "- The Eiffel Tower\n",
    "- The Louvre Museum\n",
    "- Notre-Dame Cathedral\n",
    "- Arc de Triomphe\n",
    "Paris is also home to world-class universities, financial institutions, and is one of the world's leading tourist destinations.\n",
    "\"\"\"\n",
    "\n",
    "for _ in range(4):\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"You are a helpful assistant that can generate detailed questions and answers based on the information provided below. Include specific facts and figures where relevant. Note that the current time is {random.randint(1, 12)}:{random.randint(0, 59)}\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": paris_info},\n",
    "        ],\n",
    "        response_model=Question,\n",
    "    )\n",
    "\n",
    "    print(f\"Generated Question: {resp.question}\")\n",
    "    print(f\"Answer: {resp.answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply introducing variation in the prompt itself was not enough to get a diverse set of questions. In this specific case, we had two unique questions, with the rest being the same.\n",
    "\n",
    "Instead, what we need to do is to introduce smart modes of variation.\n",
    "\n",
    "For instance, in this case here, we could do the following\n",
    "\n",
    "- We could vary the tone that we use in the question\n",
    "- We could ask for different types of questions - trivia, history, science\n",
    "- We could also vary the specific details that we ask for in the question\n",
    "\n",
    "let's see this in action below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Question: What is the estimated population of Paris?\n",
      "Answer: The estimated population of Paris is 2.1 million residents.\n",
      " \n",
      "Generated Question: What are some key aspects that contribute to Paris's status as a major global center for art, fashion, gastronomy, and culture?\n",
      "Answer: Key aspects contributing to Paris's status include its rich history, iconic landmarks like the Eiffel Tower and the Louvre Museum, a vibrant arts scene, prestigious fashion houses, diverse culinary offerings, and its role as home to world-class universities and financial institutions.\n",
      " \n",
      "Generated Question: What are some of the iconic landmarks that contribute to Paris being a major global center for art, fashion, gastronomy, and culture?\n",
      "Answer: Some iconic landmarks that contribute to Paris's global cultural significance include the Eiffel Tower, the Louvre Museum, Notre-Dame Cathedral, and the Arc de Triomphe.\n",
      " \n",
      "Generated Question: Hey, what are some of the iconic landmarks in Paris that art enthusiasts should definitely check out?\n",
      "Answer: Some must-visit iconic landmarks in Paris for art lovers include the Eiffel Tower, the Louvre Museum, Notre-Dame Cathedral, and Arc de Triomphe.\n",
      " \n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import instructor\n",
    "from pydantic import BaseModel\n",
    "import random\n",
    "\n",
    "client = instructor.from_openai(OpenAI())\n",
    "\n",
    "\n",
    "class Question(BaseModel):\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Sample information about Paris\n",
    "paris_info = \"\"\"\n",
    "Paris is the capital and largest city of France, with an estimated population of 2.1 million residents.\n",
    "Located in northern France, it is a major global center for art, fashion, gastronomy and culture.\n",
    "The city is known for its iconic landmarks including:\n",
    "- The Eiffel Tower\n",
    "- The Louvre Museum\n",
    "- Notre-Dame Cathedral\n",
    "- Arc de Triomphe\n",
    "Paris is also home to world-class universities, financial institutions, and is one of the world's leading tourist destinations.\n",
    "\"\"\"\n",
    "# Define specific questions about landmarks and attractions\n",
    "questions = [\"historical\", \"cultural\", \"geographical\", \"art\"]\n",
    "tones = [\"curt\", \"formal\", \"technical\", \"casual\"]\n",
    "for question, tone in zip(questions, tones):\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"You're a helpful assistant that can generate detailed questions and answers based on the information provided below. Make sure that the question you're generated is a question pertaining to {question} and written in a {tone} tone.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": f\"Context: {paris_info}\"},\n",
    "        ],\n",
    "        response_model=Question,\n",
    "    )\n",
    "\n",
    "    print(f\"Generated Question: {resp.question}\")\n",
    "    print(f\"Answer: {resp.answer}\")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By being more tactful about the variation that we introduce, we can generate a much more diverse set of questions. This is useful for us because it allows us to generate a much more representative set of questions that we can use to evaluate our retrieval.\n",
    "\n",
    "Now that we've seen what are some of the issues behind using synthetic questions and what they are, let's see how we can generate our first set of synthetic questions using the items in our dataset for reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Synthetic Questions\n",
    "\n",
    "Going back to the previous section, what we want is to generate a set of questions that are going to include a mix of the following conditions\n",
    "\n",
    "- Questions about price\n",
    "- Questions about material\n",
    "- Specific occasions that we'd like to use the product for\n",
    "- Availability constraints\n",
    "\n",
    "Let's see how we can do so for a single item in our dataset by reading in an item from our local `lancedb` database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lancedb import connect\n",
    "\n",
    "db = connect(\"./lancedb\")\n",
    "table = db.open_table(\"items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1,\n",
       " 'title': 'Lace Detail Sleeveless Top',\n",
       " 'description': \"Elevate your casual wardrobe with this elegant sleeveless top featuring intricate lace detailing at the neckline. Perfect for both day and night, it's crafted from a soft, breathable fabric for all-day comfort.\",\n",
       " 'brand': 'H&M',\n",
       " 'category': 'Tops',\n",
       " 'product_type': 'Tank Tops',\n",
       " 'attributes': '[{\"name\": \"Sleeve Length\", \"value\": \"Sleeveless\"}, {\"name\": \"Neckline\", \"value\": \"Crew Neck\"}]',\n",
       " 'material': 'Cotton',\n",
       " 'pattern': 'Solid',\n",
       " 'price': 181.04,\n",
       " 'vector': array([ 0.09296898,  0.04098858, -0.00206752, ..., -0.00216847,\n",
       "         0.02412327, -0.02827667], shape=(1536,), dtype=float32),\n",
       " 'in_stock': False}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = table.to_pandas().head(4)\n",
    "item = df.iloc[0].to_dict()\n",
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import instructor\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class UserQuery(BaseModel):\n",
    "    chain_of_thought: str\n",
    "    query: str\n",
    "\n",
    "\n",
    "async def generate_synthetic_question(client: instructor.AsyncInstructor, item: dict):\n",
    "    condition = [\n",
    "        \"price\",\n",
    "        \"material\",\n",
    "        \"occasions to wear the outfit for\",\n",
    "        \"whether it's in stock\",\n",
    "    ]\n",
    "    return await client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"\n",
    "Generate a natural shopping query where this item would be the perfect recommendation. This query should be a query about the {{ query_type }} of this specific item.\n",
    "\n",
    "Item Details:\n",
    "- Title: {{title}}\n",
    "- Description: {{description}}\n",
    "- Brand: {{brand}}\n",
    "- Material: {{material}}\n",
    "- Pattern: {{pattern}}\n",
    "- Attributes: {{attributes}}\n",
    "- Price: {{price}}\n",
    "- Category: {{category}}\n",
    "- Subcategory: {{subcategory}}\n",
    "- Product Type: {{product_type}}\n",
    "- In Stock : {{ stock_status }}\n",
    "\n",
    "Make sure that \n",
    "\n",
    "Requirements:\n",
    "- Query should be 20-30 words\n",
    "- Conversational tone\n",
    "- The query should describe the aspects above that make the item above a perfect match for the user's requirements\n",
    "- Try to mention things which might be synonyms for the item and avoid mentioning it directly. Instead we should use specific attributes that the item has in order to make it a good fit. Make sure to use the exact attribute name so that it's unambigious\n",
    "- for the price range, keep it to 15 bucks on either side of the price max\n",
    "- Do not mention the item's name or brand in the query itself\n",
    "\n",
    "For an office blouse that costs $120:\n",
    "\"Need something elegant for my new corporate job. Looking for a silk top with long sleeves and a modest neckline, under $150.\" ( Within the price range here )\n",
    "\n",
    "For casual wear that costs $65:\n",
    "\"Shopping for my weekend brunches. Need a cotton top that's both comfy and stylish, maybe with some interesting pattern - ideally something between 40-100 bucks if possible.\" ( 65 is less than 69 )\n",
    "\n",
    "Remember: The query should describe what someone would be looking for if this exact item would be their perfect match!\n",
    "\"\"\",\n",
    "            }\n",
    "        ],\n",
    "        context={\n",
    "            \"query_type\": random.choice(condition),\n",
    "            \"stock_status\": item[\"in_stock\"],\n",
    "            \"title\": item[\"title\"],\n",
    "            \"description\": item[\"description\"],\n",
    "            \"brand\": item[\"brand\"],\n",
    "            \"material\": item[\"material\"],\n",
    "            \"pattern\": item[\"pattern\"],\n",
    "            \"attributes\": item[\"attributes\"],\n",
    "            \"price\": item[\"price\"],\n",
    "            \"category\": item[\"category\"],\n",
    "            \"product_type\": item[\"product_type\"],\n",
    "        },\n",
    "        response_model=UserQuery,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">UserQuery</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">chain_of_thought</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"The query should focus on finding a casual top that is breathable and comfortable, perfect </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">for daily wear, with a budget around the item's price. Using the specific attributes such as the material (Cotton),</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the sleeveless design, and a crew neckline.\"</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">query</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Looking for a soft and breathable casual tank top. Must be sleeveless with a crew neck design. Budget is</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">around $165-$195. Any recommendations?'</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mUserQuery\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mchain_of_thought\u001b[0m=\u001b[32m\"The\u001b[0m\u001b[32m query should focus on finding a casual top that is breathable and comfortable, perfect \u001b[0m\n",
       "\u001b[32mfor daily wear, with a budget around the item's price. Using the specific attributes such as the material \u001b[0m\u001b[32m(\u001b[0m\u001b[32mCotton\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\u001b[0m\n",
       "\u001b[32mthe sleeveless design, and a crew neckline.\"\u001b[0m,\n",
       "    \u001b[33mquery\u001b[0m=\u001b[32m'Looking for a soft and breathable casual tank top. Must be sleeveless with a crew neck design. Budget is\u001b[0m\n",
       "\u001b[32maround $165-$195. Any recommendations?'\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import instructor\n",
    "from openai import AsyncOpenAI\n",
    "from rich import print\n",
    "\n",
    "client = instructor.from_openai(AsyncOpenAI())\n",
    "\n",
    "print(await generate_synthetic_question(client, item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we've generated our first synthetic question! We've varied the query types and the item itself so that the generated queries are diverse and representative of the items in our dataset.\n",
    "\n",
    "Let's now shift our attention to what metrics we should use to evaluate the quality of our retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Metrics\n",
    "\n",
    "Let's start by looking at a few metrics that we can use to evaluate the quality of our retrieval.\n",
    "\n",
    "### Key Retrieval Metrics\n",
    "\n",
    "**Precision** measures how many of our retrieved items are actually relevant:\n",
    "\n",
    "$$ \\text{Precision} = \\frac{\\text{Number of Relevant Items Retrieved}}{\\text{Total Number of Retrieved Items}} $$ \n",
    "\n",
    "For example, if your system retrieves 10 documents but only 5 are relevant, that's 50% precision. Low precision indicates your system is wasting resources processing irrelevant content.\n",
    "\n",
    "**Recall** measures how many of the total relevant items we managed to find:\n",
    "\n",
    "$$ \\text{Recall} = \\frac{\\text{Number of Relevant Items Retrieved}}{\\text{Total Number of Relevant Items}} $$ \n",
    "\n",
    "If there are 20 relevant documents in your database but you only retrieve 10 of them, that's 50% recall. Low recall suggests you're missing important information.\n",
    "\n",
    "**Mean Reciprocal Rank (MRR)** measures the average reciprocal rank of the first relevant document:\n",
    "\n",
    "$$ \\text{MRR} = \\frac{\\sum\\_{i=1}^{n} \\frac{1}{rank(i)}}{n} $$\n",
    "\n",
    "The higher the MRR, the better. It penalizes systems that retrieve irrelevant documents early in the list.\n",
    "\n",
    "In practice, we often measure these metrics at specific cutoff points (like top-5 or top-10 results), denoted as Precision@K or Recall@K. These cut-off points are always going to be determined by some specific business or product requirement.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mrr(predictions: list[str], gt: list[str]):\n",
    "    mrr = 0\n",
    "    for label in gt:\n",
    "        if label in predictions:\n",
    "            # Find the relevant item that has the smallest index\n",
    "            mrr = max(mrr, 1 / (predictions.index(label) + 1))\n",
    "    return mrr\n",
    "\n",
    "\n",
    "def calculate_recall(predictions: list[str], gt: list[str]):\n",
    "    # Calculate the proportion of relevant items that were retrieved\n",
    "    return len([label for label in gt if label in predictions]) / len(gt)\n",
    "\n",
    "\n",
    "def calculate_precision(predictions: list[str], gt: list[str]):\n",
    "    # Calculate the proportion of retrieved items that are relevant\n",
    "    return len([label for label in predictions if label in gt]) / len(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd ideally also like to compute the precision and recall at a variety of different cutoff points. Let's see how we can do this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools \n",
    "\n",
    "def get_metrics_at_k(\n",
    "    metrics: list[str],\n",
    "    sizes: list[int],\n",
    "):\n",
    "    metric_to_score_fn = {\n",
    "        \"mrr\": calculate_mrr,\n",
    "        \"recall\": calculate_recall,\n",
    "    }\n",
    "\n",
    "    for metric in metrics:\n",
    "        if metric not in metric_to_score_fn:\n",
    "            raise ValueError(f\"Metric {metric} not supported\")\n",
    "\n",
    "    eval_metrics = [(metric, metric_to_score_fn[metric]) for metric in metrics]\n",
    "\n",
    "    return {\n",
    "        f\"{metric_name}@{size}\": lambda predictions, gt, m=metric_fn, s=size: (\n",
    "            lambda p, g: m(p[:s], g)\n",
    "        )(predictions, gt)\n",
    "        for (metric_name, metric_fn), size in itertools.product(eval_metrics, sizes)\n",
    "    }\n",
    "\n",
    "metrics = get_metrics_at_k(metrics=[\"mrr\", \"recall\"], sizes=[5, 10, 25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then compute apply this metrics list on a set of retrieved items as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mrr@5': 0,\n",
       " 'mrr@10': 0.1,\n",
       " 'mrr@25': 0.1,\n",
       " 'recall@5': 0.0,\n",
       " 'recall@10': 1.0,\n",
       " 'recall@25': 1.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "retrieved_item_ids = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
    "desired_item = [10]\n",
    "\n",
    "scores = {metric: score_fn(retrieved_item_ids, desired_item) for metric, score_fn in metrics.items()}\n",
    "scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we've computed the metrics for a variety of different cutoff points. This is a function that we can parameterise and apply to calculate mrr and recall at a variety of different cutoff points.\n",
    "\n",
    "let's now see how we can apply this to a single query below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's a good fit for our use case?\n",
    "\n",
    "In our RAG application, we're going to be using `recall` and `MRR` as our primary metrics. This is for two reasons\n",
    "\n",
    "1. **Recall** : We want to make sure that for the synthetic question we generate for that specific item, we're able to retrieve it. Recall will determine whether this is the case for us\n",
    "\n",
    "2. **MRR** : More often than not, we're also going to be displaying these suggested items to the user. In this case, we'd want to make sure that the relevant item is ranked as highly as possible so that the user has a high chance of clicking on it. MRR will help us understand how well we're able to rank the relevant item\n",
    "\n",
    "For the remainder of this section, we're going to be looking at how we can use these metrics to compute a baseline for our retrieval system. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing a Baseline\n",
    "\n",
    "### Evaluating A Single Query\n",
    "\n",
    "We've generated a small dataset of synthetic questions ahead of time so that we don't need to generate them on the fly. Let's read in the dataset from the `queries.json` file and then use them to compute our baseline metrics.\n",
    "\n",
    "Let's see how we might evaluate the recall and MRR for a single query. We'll use the query at index 3 in our dataset which our retrieval system will struggle with slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load in queries that we generated previously\n",
    "with open(\"queries.json\", \"r\") as f:\n",
    "    queries = [json.loads(line) for line in f]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Searching for a cropped cotton top with short sleeves and a stylish turtleneck for casual outings and everyday wear, ideally priced below $400.',\n",
       " 'title': \"Fila Women's Cropped Logo T-Shirt\",\n",
       " 'brand': 'Fila',\n",
       " 'description': \"Elevate your casual look with the Fila Women's Cropped Logo T-Shirt. Featuring the iconic Fila logo in a sleek design, this short-sleeve turtleneck adds a touch of sporty elegance to any ensemble.\",\n",
       " 'category': 'Women',\n",
       " 'subcategory': 'Tops',\n",
       " 'product_type': 'T-Shirts',\n",
       " 'attributes': '[{\"name\": \"Sleeve Length\", \"value\": \"Short Sleeve\"}, {\"name\": \"Neckline\", \"value\": \"Turtleneck\"}, {\"name\": \"Fit\", \"value\": \"Cropped\"}]',\n",
       " 'material': 'Cotton',\n",
       " 'pattern': 'Solid',\n",
       " 'id': 4,\n",
       " 'price': 374.89,\n",
       " 'occasions': '[\"Everyday Wear\", \"Casual Outings\", \"Smart Casual\", \"Activewear\", \"Beachwear\", \"Loungewear\", \"Travel\"]'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see how well our retrieval system performs of this specific query. We'll do so by fetching the top 25 items from our retrieval system and then computing the recall and MRR at a variety of cutoff points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_query = queries[3]\n",
    "retrieved_item_ids = [item['id'] for item in table.search(target_query[\"query\"]).limit(25).to_list()]\n",
    "desired_item = [target_query[\"id\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've fetched the top 25 items from our retrieval system, we can then compute the recall and MRR at a variety of cutoff points as seen below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mrr@5': 0,\n",
       " 'mrr@10': 0,\n",
       " 'mrr@15': 0,\n",
       " 'mrr@20': 0,\n",
       " 'mrr@25': 0,\n",
       " 'recall@5': 0.0,\n",
       " 'recall@10': 0.0,\n",
       " 'recall@15': 0.0,\n",
       " 'recall@20': 0.0,\n",
       " 'recall@25': 0.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = get_metrics_at_k(metrics=[\"mrr\", \"recall\"], sizes=[5, 10, 15, 20, 25])\n",
    "scores = {metric: score_fn(retrieved_item_ids, desired_item) for metric, score_fn in metrics.items()}\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our retrieval system wasn't able to retrieve the relevant items for this specific query itself. Let's take a closer look to see what was retrieved and what the query was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Searching for a cropped cotton top with short sleeves and a stylish turtleneck for casual outings and everyday \n",
       "wear, ideally priced below $<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">400</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Searching for a cropped cotton top with short sleeves and a stylish turtleneck for casual outings and everyday \n",
       "wear, ideally priced below $\u001b[1;36m400\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich import print\n",
    "\n",
    "print(target_query['query'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "      <th>product_type</th>\n",
       "      <th>price</th>\n",
       "      <th>material</th>\n",
       "      <th>attributes</th>\n",
       "      <th>_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67</td>\n",
       "      <td>Women's Long Sleeve Turtleneck Top</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Sweaters</td>\n",
       "      <td>154.75</td>\n",
       "      <td>Cotton</td>\n",
       "      <td>[{\"name\": \"Sleeve Length\", \"value\": \"Long Slee...</td>\n",
       "      <td>0.898017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26</td>\n",
       "      <td>Striped Long Sleeve Top</td>\n",
       "      <td>Tops</td>\n",
       "      <td>T-Shirts</td>\n",
       "      <td>81.09</td>\n",
       "      <td>Cotton</td>\n",
       "      <td>[{\"name\": \"Sleeve Length\", \"value\": \"Long Slee...</td>\n",
       "      <td>0.901962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>73</td>\n",
       "      <td>Smocked Button Front Crop Top</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Tank Tops</td>\n",
       "      <td>355.15</td>\n",
       "      <td>Cotton</td>\n",
       "      <td>[{\"name\": \"Sleeve Length\", \"value\": \"Sleeveles...</td>\n",
       "      <td>0.919500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>Women's Black Wrap Crop Top</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "      <td>99.28</td>\n",
       "      <td>Cotton</td>\n",
       "      <td>[{\"name\": \"Sleeve Length\", \"value\": \"Long Slee...</td>\n",
       "      <td>0.928390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>172</td>\n",
       "      <td>Patterned Long Sleeve Top</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Sweaters</td>\n",
       "      <td>382.99</td>\n",
       "      <td>Cotton</td>\n",
       "      <td>[{\"name\": \"Sleeve Length\", \"value\": \"Long Slee...</td>\n",
       "      <td>0.928833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Plaid Crop Top</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Tank Tops</td>\n",
       "      <td>261.05</td>\n",
       "      <td>Cotton</td>\n",
       "      <td>[{\"name\": \"Sleeve Length\", \"value\": \"Sleeveles...</td>\n",
       "      <td>0.933500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>59</td>\n",
       "      <td>Women's Long Sleeve Turtleneck Top</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Sweaters</td>\n",
       "      <td>225.63</td>\n",
       "      <td>Cotton</td>\n",
       "      <td>[{\"name\": \"Sleeve Length\", \"value\": \"Long Slee...</td>\n",
       "      <td>0.939630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>177</td>\n",
       "      <td>Sleeveless Button-Down Blouse</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "      <td>353.57</td>\n",
       "      <td>Cotton</td>\n",
       "      <td>[{\"name\": \"Sleeve Length\", \"value\": \"Sleeveles...</td>\n",
       "      <td>0.942118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>85</td>\n",
       "      <td>Women's Sleeveless Tie-Waist Crop Top</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Tank Tops</td>\n",
       "      <td>163.79</td>\n",
       "      <td>Spandex</td>\n",
       "      <td>[{\"name\": \"Sleeve Length\", \"value\": \"Sleeveles...</td>\n",
       "      <td>0.944312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>130</td>\n",
       "      <td>Women's Cutout Cropped Top</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "      <td>222.68</td>\n",
       "      <td>Cotton</td>\n",
       "      <td>[{\"name\": \"Sleeve Length\", \"value\": \"Long Slee...</td>\n",
       "      <td>0.946629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>182</td>\n",
       "      <td>Ribbed Short Sleeve Top</td>\n",
       "      <td>Tops</td>\n",
       "      <td>T-Shirts</td>\n",
       "      <td>56.94</td>\n",
       "      <td>Cotton</td>\n",
       "      <td>[{\"name\": \"Sleeve Length\", \"value\": \"Short Sle...</td>\n",
       "      <td>0.951020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>21</td>\n",
       "      <td>Women's V-Neck T-Shirt</td>\n",
       "      <td>Tops</td>\n",
       "      <td>T-Shirts</td>\n",
       "      <td>223.10</td>\n",
       "      <td>Cotton</td>\n",
       "      <td>[{\"name\": \"Sleeve Length\", \"value\": \"Short Sle...</td>\n",
       "      <td>0.967851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>151</td>\n",
       "      <td>Women's White Short Sleeve V-Neck Top</td>\n",
       "      <td>Tops</td>\n",
       "      <td>T-Shirts</td>\n",
       "      <td>240.17</td>\n",
       "      <td>Cotton</td>\n",
       "      <td>[{\"name\": \"Sleeve Length\", \"value\": \"Short Sle...</td>\n",
       "      <td>0.975548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>138</td>\n",
       "      <td>Sleeveless Black Top</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Tank Tops</td>\n",
       "      <td>298.01</td>\n",
       "      <td>Polyester</td>\n",
       "      <td>[{\"name\": \"Sleeve Length\", \"value\": \"Sleeveles...</td>\n",
       "      <td>0.978636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>152</td>\n",
       "      <td>Long Sleeve Scoop Neck Top</td>\n",
       "      <td>Tops</td>\n",
       "      <td>T-Shirts</td>\n",
       "      <td>15.64</td>\n",
       "      <td>Cotton</td>\n",
       "      <td>[{\"name\": \"Sleeve Length\", \"value\": \"Long Slee...</td>\n",
       "      <td>0.979926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>66</td>\n",
       "      <td>Black Lace Long Sleeve Top</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "      <td>49.44</td>\n",
       "      <td>Polyester</td>\n",
       "      <td>[{\"name\": \"Sleeve Length\", \"value\": \"Long Slee...</td>\n",
       "      <td>0.984351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>108</td>\n",
       "      <td>Lace-Up Cropped Blouse</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "      <td>158.65</td>\n",
       "      <td>Cotton</td>\n",
       "      <td>[{\"name\": \"Sleeve Length\", \"value\": \"Short Sle...</td>\n",
       "      <td>0.986709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>133</td>\n",
       "      <td>Women's Striped Long Sleeve Top</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Sweaters</td>\n",
       "      <td>196.02</td>\n",
       "      <td>Cotton</td>\n",
       "      <td>[{\"name\": \"Sleeve Length\", \"value\": \"Long Slee...</td>\n",
       "      <td>0.987125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>82</td>\n",
       "      <td>Calvin Klein Women's Contrast Collar T-Shirt</td>\n",
       "      <td>Tops</td>\n",
       "      <td>T-Shirts</td>\n",
       "      <td>272.54</td>\n",
       "      <td>Cotton</td>\n",
       "      <td>[{\"name\": \"Sleeve Length\", \"value\": \"Short Sle...</td>\n",
       "      <td>0.988294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>121</td>\n",
       "      <td>Women's 3/4 Sleeve Knit Top</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Sweaters</td>\n",
       "      <td>384.26</td>\n",
       "      <td>Cotton</td>\n",
       "      <td>[{\"name\": \"Sleeve Length\", \"value\": \"3/4 Sleev...</td>\n",
       "      <td>0.989671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                         title category product_type  \\\n",
       "0    67            Women's Long Sleeve Turtleneck Top     Tops     Sweaters   \n",
       "1    26                       Striped Long Sleeve Top     Tops     T-Shirts   \n",
       "2    73                 Smocked Button Front Crop Top     Tops    Tank Tops   \n",
       "3    44                   Women's Black Wrap Crop Top     Tops      Blouses   \n",
       "4   172                     Patterned Long Sleeve Top     Tops     Sweaters   \n",
       "5     5                                Plaid Crop Top     Tops    Tank Tops   \n",
       "6    59            Women's Long Sleeve Turtleneck Top     Tops     Sweaters   \n",
       "7   177                 Sleeveless Button-Down Blouse     Tops      Blouses   \n",
       "8    85         Women's Sleeveless Tie-Waist Crop Top     Tops    Tank Tops   \n",
       "9   130                    Women's Cutout Cropped Top     Tops      Blouses   \n",
       "10  182                       Ribbed Short Sleeve Top     Tops     T-Shirts   \n",
       "11   21                        Women's V-Neck T-Shirt     Tops     T-Shirts   \n",
       "12  151         Women's White Short Sleeve V-Neck Top     Tops     T-Shirts   \n",
       "13  138                          Sleeveless Black Top     Tops    Tank Tops   \n",
       "14  152                    Long Sleeve Scoop Neck Top     Tops     T-Shirts   \n",
       "15   66                    Black Lace Long Sleeve Top     Tops      Blouses   \n",
       "16  108                        Lace-Up Cropped Blouse     Tops      Blouses   \n",
       "17  133               Women's Striped Long Sleeve Top     Tops     Sweaters   \n",
       "18   82  Calvin Klein Women's Contrast Collar T-Shirt     Tops     T-Shirts   \n",
       "19  121                   Women's 3/4 Sleeve Knit Top     Tops     Sweaters   \n",
       "\n",
       "     price   material                                         attributes  \\\n",
       "0   154.75     Cotton  [{\"name\": \"Sleeve Length\", \"value\": \"Long Slee...   \n",
       "1    81.09     Cotton  [{\"name\": \"Sleeve Length\", \"value\": \"Long Slee...   \n",
       "2   355.15     Cotton  [{\"name\": \"Sleeve Length\", \"value\": \"Sleeveles...   \n",
       "3    99.28     Cotton  [{\"name\": \"Sleeve Length\", \"value\": \"Long Slee...   \n",
       "4   382.99     Cotton  [{\"name\": \"Sleeve Length\", \"value\": \"Long Slee...   \n",
       "5   261.05     Cotton  [{\"name\": \"Sleeve Length\", \"value\": \"Sleeveles...   \n",
       "6   225.63     Cotton  [{\"name\": \"Sleeve Length\", \"value\": \"Long Slee...   \n",
       "7   353.57     Cotton  [{\"name\": \"Sleeve Length\", \"value\": \"Sleeveles...   \n",
       "8   163.79    Spandex  [{\"name\": \"Sleeve Length\", \"value\": \"Sleeveles...   \n",
       "9   222.68     Cotton  [{\"name\": \"Sleeve Length\", \"value\": \"Long Slee...   \n",
       "10   56.94     Cotton  [{\"name\": \"Sleeve Length\", \"value\": \"Short Sle...   \n",
       "11  223.10     Cotton  [{\"name\": \"Sleeve Length\", \"value\": \"Short Sle...   \n",
       "12  240.17     Cotton  [{\"name\": \"Sleeve Length\", \"value\": \"Short Sle...   \n",
       "13  298.01  Polyester  [{\"name\": \"Sleeve Length\", \"value\": \"Sleeveles...   \n",
       "14   15.64     Cotton  [{\"name\": \"Sleeve Length\", \"value\": \"Long Slee...   \n",
       "15   49.44  Polyester  [{\"name\": \"Sleeve Length\", \"value\": \"Long Slee...   \n",
       "16  158.65     Cotton  [{\"name\": \"Sleeve Length\", \"value\": \"Short Sle...   \n",
       "17  196.02     Cotton  [{\"name\": \"Sleeve Length\", \"value\": \"Long Slee...   \n",
       "18  272.54     Cotton  [{\"name\": \"Sleeve Length\", \"value\": \"Short Sle...   \n",
       "19  384.26     Cotton  [{\"name\": \"Sleeve Length\", \"value\": \"3/4 Sleev...   \n",
       "\n",
       "    _distance  \n",
       "0    0.898017  \n",
       "1    0.901962  \n",
       "2    0.919500  \n",
       "3    0.928390  \n",
       "4    0.928833  \n",
       "5    0.933500  \n",
       "6    0.939630  \n",
       "7    0.942118  \n",
       "8    0.944312  \n",
       "9    0.946629  \n",
       "10   0.951020  \n",
       "11   0.967851  \n",
       "12   0.975548  \n",
       "13   0.978636  \n",
       "14   0.979926  \n",
       "15   0.984351  \n",
       "16   0.986709  \n",
       "17   0.987125  \n",
       "18   0.988294  \n",
       "19   0.989671  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.search(target_query[\"query\"]).select([\"id\", \"title\",\"category\", \"product_type\",\"price\",\"material\",\"attributes\"]).limit(25).to_pandas().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that while the price was adhered to, we've completely missed the mark on the other aspects. \n",
    "\n",
    "1. We've got a mix of different materials - even though most of cotton, we do have some Polyester and Spandex in the mix\n",
    "2. We've got a mix of different product types - we've got a t-shirt, a dress and a skirt when what we wanted was a T-Shirt\n",
    "3. We've also got a mix of different attributes - we wanted short sleeved shirts but we've got some long sleeved shirts in the mix as well\n",
    "\n",
    "As a result, our original item didn't appear in the top 25 results. Let's now compute the recall and MRR for all of our given queries and see how we perform as a whole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Our Baseline\n",
    "\n",
    "We'll use pandas to store the results of our computations and then dump the results into a simple .csv file for future reference. To run our code in Parallel, we'll use a ThreadPoolExecutor to run these async calls in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mrr@5': 1.0,\n",
       " 'mrr@10': 1.0,\n",
       " 'mrr@15': 1.0,\n",
       " 'mrr@20': 1.0,\n",
       " 'mrr@25': 1.0,\n",
       " 'recall@5': 1.0,\n",
       " 'recall@10': 1.0,\n",
       " 'recall@15': 1.0,\n",
       " 'recall@20': 1.0,\n",
       " 'recall@25': 1.0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lancedb.table import Table\n",
    "\n",
    "def retrieve_items(table:Table,item:dict, metrics:dict):\n",
    "    retrieved_item_ids = [item['id'] for item in table.search(item['query']).limit(25).to_list()]\n",
    "    desired_item = [item['id']]\n",
    "    return {metric: score_fn(retrieved_item_ids, desired_item) for metric, score_fn in metrics.items()}\n",
    "\n",
    "\n",
    "db = connect(\"./lancedb\")\n",
    "table = db.open_table(\"items\")\n",
    "retrieve_items(table,queries[2],metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mrr@5        0.471930\n",
       "mrr@10       0.489756\n",
       "mrr@15       0.491635\n",
       "mrr@20       0.494731\n",
       "mrr@25       0.494731\n",
       "recall@5     0.631579\n",
       "recall@10    0.763158\n",
       "recall@15    0.789474\n",
       "recall@20    0.842105\n",
       "recall@25    0.842105\n",
       "dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pandas as pd\n",
    "\n",
    "# Create a ThreadPoolExecutor to run queries in parallel\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    # Map the retrieve_items function across all queries\n",
    "    results = list(executor.map(\n",
    "        lambda q: retrieve_items(table, q, metrics),\n",
    "        queries\n",
    "    ))\n",
    "\n",
    "# Convert results to DataFrame for analysis\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good baseline for us to start with. In the next section, we'll see how we can use metadata filtering to improve these results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
